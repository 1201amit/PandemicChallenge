{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from dateutil.parser import parse \n",
    "import datetime\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_codes = ['ABW','AFG','AGO','ALB','AND','ARE','ARG','AUS','AUT','AZE','BDI','BEL','BEN','BFA','BGD','BGR','BHR','BHS','BIH','BLR','BLZ','BMU','BOL','BRA','BRB','BRN','BTN','BWA','CAF','CAN','CHE','CHL','CHN','CIV','CMR','COD','COG','COL','COM','CPV','CRI','CUB','CYP','CZE','DEU','DJI','DMA','DNK','DOM','DZA','ECU','EGY','ERI','ESP','EST','ETH','FIN','FJI','FRA','FRO','GAB','GBR','GEO','GHA','GIN','GMB','GRC','GRL','GTM','GUM','GUY','HKG','HND','HRV','HTI','HUN','IDN','IND','IRL','IRN','IRQ','ISL','ISR','ITA','JAM','JOR','JPN','KAZ','KEN','KGZ','KHM','KOR','KWT','LAO','LBN','LBR','LBY','LKA','LSO','LTU','LUX','LVA','MAC','MAR','MCO','MDA','MDG','MEX','MLI','MMR','MNG','MOZ','MRT','MUS','MWI','MYS','NAM','NER','NGA','NIC','NLD','NOR','NPL','NZL','OMN','PAK','PAN','PER','PHL','PNG','POL','PRI','PRT','PRY','PSE','QAT','RKS','ROU','RUS','RWA','SAU','SDN','SEN','SGP','SLB','SLE','SLV','SMR','SOM','SRB','SSD','SUR','SVK','SVN','SWE','SWZ','SYC','SYR','TCD','TGO','THA','TJK','TKM','TLS','TTO','TUN','TUR','TWN','TZA','UGA','UKR','URY','USA','UZB','VEN','VIR','VNM','VUT','YEM','ZAF','ZMB','ZWE']\n",
    "filenames = [\"c1_school_closing.csv\", \"c2_workplace_closing.csv\", \"c3_cancel_public_events.csv\", \"c4_restrictions_on_gatherings.csv\", \"c5_close_public_transport.csv\", \"c6_stay_at_home_requirements.csv\", \"c7_movementrestrictions.csv\", \"c8_internationaltravel.csv\", \"confirmed_cases.csv\", \"h1_public_information_campaigns.csv\", \"h2_testing_policy.csv\", \"h3_contact_tracing.csv\", \"h6_facial_coverings.csv\"]\n",
    "\n",
    "def dateConvertor(date):\n",
    "    dt = parse(date)\n",
    "    date = dt.strftime('%Y-%m-%d')\n",
    "    return date\n",
    "\n",
    "country_code2id = {}\n",
    "for i in range(len(country_codes)):\n",
    "    country_code2id[country_codes[i]] = i \n",
    "\n",
    "# date extraction\n",
    "npi_date = pd.DataFrame({})\n",
    "npi_date['Date'] = pd.read_csv(os.path.join('timeseries', filenames[0])).keys()[3:]\n",
    "npi_date['Date'] = npi_date['Date'].apply(dateConvertor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataframes = {} \n",
    "\n",
    "countries_to_extract = ['ITA','IND','USA','CHN','BRA','IRN','CAN','GBR',\n",
    "                        'FRA','ESP','BEL','DEU','NLD','MEX','TUR','SWE','ECU','RUS','PER','CHE'] # countries code for which you want data.\n",
    "\n",
    "countries_to_extract = ['IND','USA','BRA','IRN','CAN','GBR','FRA','ESP','BEL','DEU','NLD','MEX','TUR','SWE','ECU','RUS','PER']\n",
    "\n",
    "index = [country_code2id[code] for code in countries_to_extract]\n",
    "static_data = pd.read_csv(os.path.join('timeseries', 'Consolidated.csv')).T[2:][index].T\n",
    "population = static_data['Population'].to_numpy()\n",
    "\n",
    "static_data = static_data.drop('Population', axis = 1 )\n",
    "cols_to_norm = ['Density', 'Median Age']\n",
    "static_data[cols_to_norm] = static_data[cols_to_norm].apply(lambda x: (x - x.min()) / x.max()-x.min())\n",
    "static_data = static_data.to_numpy()\n",
    "tmp = static_data[:,3:]\n",
    "final_static_data = static_data[:,0:3].astype(np.float64)\n",
    "#print(final_static_data)\n",
    "for file in filenames:\n",
    "    npi_df = pd.read_csv(os.path.join('timeseries', file)).T[3:]\n",
    "    npi_df['Date'] = npi_date['Date'].values\n",
    "    npi_df.set_index('Date', drop=True, inplace=True)\n",
    "    npi_df = npi_df[index] # selecting countries \n",
    "    # npi_df = npi_df.rolling(7).mean()\n",
    "    npi_df = npi_df[64:335] # removing Jan, Feb and Dec data\n",
    "    for col in npi_df:\n",
    "        npi_df[col] = pd.to_numeric(npi_df[col], errors='coerce') # converting object to numeric \n",
    "    npi_df.interpolate(method='linear', inplace=True) # interpolate missing values \n",
    "    dataframes[file[:-4]] = npi_df\n",
    "   \n",
    "    if(file[:-4]=='confirmed_cases'):\n",
    "        npi_df = pd.read_csv(os.path.join('timeseries', file)).T[3:]\n",
    "        npi_df['Date'] = npi_date['Date'].values\n",
    "        npi_df.set_index('Date', drop=True, inplace=True)\n",
    "        npi_df = npi_df[index] # selecting countries \n",
    "        for col in npi_df:\n",
    "            npi_df[col] = pd.to_numeric(npi_df[col], errors='coerce')\n",
    "        npi_df = npi_df.interpolate(method='linear') # interpolate missing values     \n",
    "        npi_df = npi_df.rolling(7).mean()\n",
    "        npi_df = 100*npi_df.diff()/npi_df\n",
    "        npi_df = npi_df[64:335] # removing Jan, Feb and Dec data\n",
    "        dataframes['growth_rate'] = npi_df\n",
    "\n",
    "dataframes['confirmed_cases'] = 100*dataframes['confirmed_cases'].div(population)\n",
    "for col in dataframes['confirmed_cases']:\n",
    "    dataframes['confirmed_cases'][col] = pd.to_numeric(dataframes['confirmed_cases'][col], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(attributes, history, date):\n",
    "    index = dataframes['c1_school_closing'].index.get_loc(date)\n",
    "    if(history>index):\n",
    "        print('Not sufficient history')\n",
    "        sys.exit()\n",
    "    date = datetime.datetime.strptime(date, \"%Y-%m-%d\")\n",
    "    ref_date = date - datetime.timedelta(21)\n",
    "    if ref_date.month == date.month:\n",
    "        temperature = tmp[:,int(date.month)].reshape((len(countries_to_extract),1)).astype(np.float64)\n",
    "    else:\n",
    "        temperature = ((tmp[:,int(date.month)] + tmp[:,int(ref_date.month)]) / 2).reshape((len(countries_to_extract),1)).astype(np.float64)\n",
    "    data = []\n",
    "    #past_growthrates = dataframes['growth_rate'].iloc[index-history:index].values.reshape((len(countries_to_extract),21))\n",
    "    new_final_static_data = np.concatenate((final_static_data,temperature),axis=1)\n",
    "    for att in attributes:\n",
    "        temp = dataframes[att].iloc[index-history:index].values\n",
    "        if(len(data)==0):\n",
    "            data = np.asarray(temp)\n",
    "        else:\n",
    "            data = np.dstack((data, temp))\n",
    "    #x = torch.cat((torch.from_numpy(data).to(dtype=torch.double).permute(1,0,2).view(len(countries_to_extract),-1),\n",
    "                   #torch.from_numpy(new_final_static_data).to(dtype=torch.double)),dim = -1)\n",
    "    #x = torch.cat((torch.from_numpy(data).to(dtype=torch.double).permute(1,0,2).view(len(countries_to_extract),-1),\n",
    "                   #torch.from_numpy(past_growthrates).to(dtype=torch.double)),dim = -1)\n",
    "    x = torch.from_numpy(data).to(dtype=torch.double).permute(1,0,2).view(len(countries_to_extract),-1)\n",
    "    y = torch.from_numpy(dataframes['growth_rate'].iloc[index:index+7].values).to(dtype=torch.double).permute(1,0)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch \n",
    "import torch.optim as optim\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "import pandas as pd \n",
    "import os\n",
    "import numpy as np\n",
    "from dateutil.parser import parse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim , out_dim = 64):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(input_dim,256)\n",
    "        self.linear2 = torch.nn.Linear(256,512)\n",
    "        self.linear3 = torch.nn.Linear(512,256)\n",
    "        self.linear4 = torch.nn.Linear(256,128)\n",
    "        self.linear5 = torch.nn.Linear(128,out_dim)\n",
    "        self.prelu1   = torch.nn.PReLU()\n",
    "        self.prelu2   = torch.nn.PReLU()\n",
    "        self.prelu3   = torch.nn.PReLU()\n",
    "        self.prelu4   = torch.nn.PReLU()\n",
    "        self.tanh    = torch.nn.Tanh()\n",
    "        self.dropout = torch.nn.Dropout(p=0.0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.prelu1(self.linear1(x))\n",
    "        x = self.dropout(self.prelu2(self.linear2(x)))\n",
    "        x = self.dropout(self.prelu3(self.linear3(x)))\n",
    "        x = self.dropout(self.prelu4(self.linear4(x)))\n",
    "        x = self.tanh(self.linear5(x)/20)*24\n",
    "        return x.squeeze()\n",
    "\n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, output_dim, input_dim=64):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(input_dim,64)\n",
    "        self.linear2 = torch.nn.Linear(64,128)\n",
    "        self.linear3 = torch.nn.Linear(128,256)\n",
    "        self.linear4 = torch.nn.Linear(256,output_dim)\n",
    "        self.prelu1   = torch.nn.PReLU()\n",
    "        self.prelu2   = torch.nn.PReLU()\n",
    "        self.prelu3   = torch.nn.PReLU()\n",
    "        self.relu   = torch.nn.ReLU()\n",
    "        self.dropout = torch.nn.Dropout(p=0.0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.prelu1(self.linear1(x))\n",
    "        x = self.dropout(self.prelu2(self.linear2(x)))\n",
    "        x = self.dropout(self.prelu3(self.linear3(x)))\n",
    "        x = self.dropout(self.relu(self.linear4(x)))\n",
    "        return x.squeeze()\n",
    "    \n",
    "class Decoder1(torch.nn.Module):\n",
    "    def __init__(self,input_dim,output_dim=1):\n",
    "        super(Decoder1, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(input_dim,64)\n",
    "        self.linear2 = torch.nn.Linear(64,32)\n",
    "        self.linear3 = torch.nn.Linear(32,16)\n",
    "        self.linear4 = torch.nn.Linear(16,7)\n",
    "        self.prelu1   = torch.nn.PReLU()\n",
    "        self.prelu2   = torch.nn.PReLU()\n",
    "        self.prelu3   = torch.nn.PReLU()\n",
    "        self.dropout = torch.nn.Dropout(p=0.0)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.dropout(self.prelu1(self.linear1(x)))\n",
    "        x = self.dropout(self.prelu2(self.linear2(x)))\n",
    "        x = self.dropout(self.prelu3(self.linear3(x)))\n",
    "        x = self.linear4(x)\n",
    "        return x.squeeze()\n",
    "\n",
    "class Decoder2(torch.nn.Module):\n",
    "    def __init__(self,input_dim,output_dim=168):\n",
    "        super(Decoder2, self).__init__()\n",
    "        \n",
    "\n",
    "        self.linear1 = torch.nn.Linear(input_dim,64)\n",
    "        self.linear4 = torch.nn.Linear(64,64)\n",
    "        \n",
    "        self.linear2 = torch.nn.Linear(64,128)\n",
    "        self.linear3 = torch.nn.Linear(128,output_dim)\n",
    "        self.prelu1   = torch.nn.PReLU()\n",
    "        self.prelu2   = torch.nn.PReLU()\n",
    "        self.prelu3   = torch.nn.PReLU()\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(p=0.0)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.dropout(self.prelu1(self.linear1(x)))\n",
    "        x = self.dropout(self.prelu3(self.linear4(x)))\n",
    "        \n",
    "        x = self.dropout(self.prelu2(self.linear2(x)))\n",
    "        x = self.relu(self.linear3(x))\n",
    "        return x.squeeze() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([17, 252])\n",
      "torch.Size([17, 7])\n"
     ]
    }
   ],
   "source": [
    "##training,testing and validation split\n",
    "training_attributes = [\"c1_school_closing\", \"c2_workplace_closing\", \"c3_cancel_public_events\", \"c4_restrictions_on_gatherings\", \"c5_close_public_transport\", \"c6_stay_at_home_requirements\", \"c7_movementrestrictions\", \"c8_internationaltravel\", \"h1_public_information_campaigns\", \"h2_testing_policy\", \"h3_contact_tracing\", \"h6_facial_coverings\" ]\n",
    "history = 21\n",
    "train_dates = npi_date['Date'][100:300].values\n",
    "x,y = readData(attributes=training_attributes, history=history, date=train_dates[-1])\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'validation_dates' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-53e21f3a3b36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvalid_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mprev_validation_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'validation loss before training %0.4f'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mprev_validation_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SCAM train exit'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-53e21f3a3b36>\u001b[0m in \u001b[0;36mvalidation\u001b[0;34m(enc, dec1)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mdec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_dates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreadData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattributes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_attributes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_dates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'validation_dates' is not defined"
     ]
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "        \n",
    "enc = Encoder(21*12)\n",
    "dec = Decoder(21*12)\n",
    "enc.load_state_dict(torch.load('./checkpoints/enc.pth'))\n",
    "dec.load_state_dict(torch.load('./checkpoints/dec.pth'))\n",
    "# enc.apply(init_weights)\n",
    "# dec.apply(init_weights)\n",
    "\n",
    "params = list(enc.parameters())+list(dec.parameters())\n",
    "optimizer = optim.Adam(params, lr=2.5e-5, weight_decay=1e-5)\n",
    "mse_loss = torch.nn.L1Loss()\n",
    "\n",
    "def validation(enc, dec1):\n",
    "    enc.eval()\n",
    "    dec.eval() \n",
    "    valid_loss = 0\n",
    "    for i in range(len(validation_dates)):\n",
    "        x,y = readData(attributes=training_attributes, history=history, date=validation_dates[i])\n",
    "        x,y = x.float(), y.float()\n",
    "        features = dec(enc(x))\n",
    "        loss = mse_loss(features,x)\n",
    "        valid_loss += loss.item()\n",
    "    enc.train()\n",
    "    dec.train()\n",
    "    return valid_loss\n",
    "\n",
    "prev_validation_loss = validation(enc, dec)\n",
    "print('validation loss before training %0.4f'%prev_validation_loss)\n",
    "print('SCAM train exit')\n",
    "sys.exit()\n",
    "\n",
    "for epoch in range(1000):\n",
    "    np.random.shuffle(train_dates)\n",
    "    epoch_loss = 0\n",
    "    enc.train()\n",
    "    dec.train()\n",
    "    loss_t = 0.0\n",
    "    for i in range(len(train_dates)): \n",
    "        optimizer.zero_grad()\n",
    "        x,y = readData(attributes=training_attributes, history=history, date=train_dates[i])\n",
    "        x,y = x.float(), y.float()\n",
    "        x_pred = dec(enc(x+(torch.rand(x.shape)-0.5)/8))\n",
    "        loss = mse_loss(x_pred,x)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_t += loss.item()\n",
    "    \n",
    "    valid_loss = validation(enc, dec)\n",
    "    print('epoch %d | training loss %0.4f | validation loss %0.4f'%(epoch, loss_t, valid_loss))\n",
    "    \n",
    "    if (epoch+1)%10 == 0:\n",
    "        if valid_loss < prev_validation_loss:\n",
    "            print('saving weights for lower loss')\n",
    "            torch.save(enc.state_dict(), './checkpoints/enc.pth')\n",
    "            torch.save(dec.state_dict(), './checkpoints/dec.pth')\n",
    "            print('done saving')\n",
    "            prev_validation_loss = valid_loss\n",
    "        print('='*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amit/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/amit/.local/lib/python3.6/site-packages/torch/autograd/__init__.py:132: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  allow_unreachable=True)  # allow_unreachable flag\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | training loss 48.7309\n",
      "epoch 1 | training loss 43.8228\n",
      "epoch 2 | training loss 41.3564\n",
      "epoch 3 | training loss 39.6740\n",
      "epoch 4 | training loss 38.4775\n",
      "saving weights\n",
      "done saving\n",
      "==================================================\n",
      "epoch 5 | training loss 37.6657\n",
      "epoch 6 | training loss 36.9690\n",
      "epoch 7 | training loss 36.3875\n",
      "epoch 8 | training loss 35.9047\n",
      "epoch 9 | training loss 35.4820\n",
      "saving weights\n",
      "done saving\n",
      "==================================================\n",
      "epoch 10 | training loss 35.1111\n",
      "epoch 11 | training loss 34.7820\n",
      "epoch 12 | training loss 34.4975\n",
      "epoch 13 | training loss 34.2305\n",
      "epoch 14 | training loss 33.9850\n",
      "saving weights\n",
      "done saving\n",
      "==================================================\n",
      "epoch 15 | training loss 33.7875\n",
      "epoch 16 | training loss 33.5811\n",
      "epoch 17 | training loss 33.3953\n",
      "epoch 18 | training loss 33.2327\n",
      "epoch 19 | training loss 33.0778\n",
      "saving weights\n",
      "done saving\n",
      "==================================================\n",
      "epoch 20 | training loss 32.9428\n",
      "epoch 21 | training loss 32.7884\n",
      "epoch 22 | training loss 32.6632\n",
      "epoch 23 | training loss 32.5319\n",
      "epoch 24 | training loss 32.4176\n",
      "saving weights\n",
      "done saving\n",
      "==================================================\n",
      "epoch 25 | training loss 32.3159\n",
      "epoch 26 | training loss 32.2125\n",
      "epoch 27 | training loss 32.1377\n",
      "epoch 28 | training loss 32.0594\n",
      "epoch 29 | training loss 31.9703\n",
      "saving weights\n",
      "done saving\n",
      "==================================================\n",
      "epoch 30 | training loss 31.8945\n",
      "epoch 31 | training loss 31.8249\n",
      "epoch 32 | training loss 31.7421\n",
      "epoch 33 | training loss 31.6729\n",
      "epoch 34 | training loss 31.6240\n",
      "saving weights\n",
      "done saving\n",
      "==================================================\n",
      "epoch 35 | training loss 31.5485\n",
      "epoch 36 | training loss 31.4960\n",
      "epoch 37 | training loss 31.4467\n",
      "epoch 38 | training loss 31.3924\n",
      "epoch 39 | training loss 31.3341\n",
      "saving weights\n",
      "done saving\n",
      "==================================================\n",
      "epoch 40 | training loss 31.2845\n",
      "epoch 41 | training loss 31.2596\n",
      "epoch 42 | training loss 31.1968\n",
      "epoch 43 | training loss 31.1735\n",
      "epoch 44 | training loss 31.1208\n",
      "saving weights\n",
      "done saving\n",
      "==================================================\n",
      "epoch 45 | training loss 31.0808\n",
      "epoch 46 | training loss 31.0350\n",
      "epoch 47 | training loss 30.9915\n",
      "epoch 48 | training loss 30.9393\n",
      "epoch 49 | training loss 30.9205\n",
      "saving weights\n",
      "done saving\n",
      "==================================================\n",
      "epoch 50 | training loss 30.8688\n",
      "epoch 51 | training loss 30.8438\n",
      "epoch 52 | training loss 30.7966\n",
      "epoch 53 | training loss 30.7595\n",
      "epoch 54 | training loss 30.7203\n",
      "saving weights\n",
      "done saving\n",
      "==================================================\n",
      "epoch 55 | training loss 30.6867\n",
      "epoch 56 | training loss 30.6693\n",
      "epoch 57 | training loss 30.6254\n",
      "epoch 58 | training loss 30.5984\n",
      "epoch 59 | training loss 30.5613\n",
      "saving weights\n",
      "done saving\n",
      "==================================================\n",
      "epoch 60 | training loss 30.5196\n",
      "epoch 61 | training loss 30.4987\n",
      "epoch 62 | training loss 30.4702\n",
      "epoch 63 | training loss 30.4388\n",
      "epoch 64 | training loss 30.4022\n",
      "saving weights\n",
      "done saving\n",
      "==================================================\n",
      "epoch 65 | training loss 30.3858\n",
      "epoch 66 | training loss 30.3419\n",
      "epoch 67 | training loss 30.3004\n",
      "epoch 68 | training loss 30.2971\n",
      "epoch 69 | training loss 30.2594\n",
      "saving weights\n",
      "done saving\n",
      "==================================================\n",
      "epoch 70 | training loss 30.2060\n",
      "epoch 71 | training loss 30.1901\n",
      "epoch 72 | training loss 30.1492\n",
      "epoch 73 | training loss 30.1091\n",
      "epoch 74 | training loss 30.0892\n",
      "saving weights\n",
      "done saving\n",
      "==================================================\n",
      "epoch 75 | training loss 30.0533\n",
      "epoch 76 | training loss 30.0231\n",
      "epoch 77 | training loss 29.9924\n",
      "epoch 78 | training loss 29.9670\n",
      "epoch 79 | training loss 29.9319\n",
      "saving weights\n",
      "done saving\n",
      "==================================================\n",
      "epoch 80 | training loss 29.8989\n",
      "epoch 81 | training loss 29.8620\n",
      "epoch 82 | training loss 29.8383\n",
      "epoch 83 | training loss 29.8037\n",
      "epoch 84 | training loss 29.7975\n",
      "saving weights\n",
      "done saving\n",
      "==================================================\n",
      "epoch 85 | training loss 29.7538\n",
      "epoch 86 | training loss 29.7076\n",
      "epoch 87 | training loss 29.6856\n",
      "epoch 88 | training loss 29.6436\n",
      "epoch 89 | training loss 29.6023\n",
      "saving weights\n",
      "done saving\n",
      "==================================================\n",
      "epoch 90 | training loss 29.5794\n",
      "epoch 91 | training loss 29.5584\n",
      "epoch 92 | training loss 29.5215\n",
      "epoch 93 | training loss 29.5052\n",
      "epoch 94 | training loss 29.4620\n",
      "saving weights\n",
      "done saving\n",
      "==================================================\n",
      "epoch 95 | training loss 29.4391\n",
      "epoch 96 | training loss 29.4184\n",
      "epoch 97 | training loss 29.3814\n",
      "epoch 98 | training loss 29.3584\n",
      "epoch 99 | training loss 29.3301\n",
      "saving weights\n",
      "done saving\n",
      "==================================================\n",
      "epoch 100 | training loss 29.2823\n",
      "epoch 101 | training loss 29.2784\n",
      "epoch 102 | training loss 29.2454\n",
      "epoch 103 | training loss 29.2381\n",
      "epoch 104 | training loss 29.1994\n",
      "saving weights\n",
      "done saving\n",
      "==================================================\n",
      "epoch 105 | training loss 29.1680\n",
      "epoch 106 | training loss 29.1434\n",
      "epoch 107 | training loss 29.1116\n",
      "epoch 108 | training loss 29.0939\n",
      "epoch 109 | training loss 29.0580\n",
      "saving weights\n",
      "done saving\n",
      "==================================================\n",
      "epoch 110 | training loss 29.0253\n",
      "epoch 111 | training loss 29.0026\n",
      "epoch 112 | training loss 28.9686\n",
      "epoch 113 | training loss 28.9350\n",
      "epoch 114 | training loss 28.9200\n",
      "saving weights\n",
      "done saving\n",
      "==================================================\n",
      "epoch 115 | training loss 28.8866\n",
      "epoch 116 | training loss 28.8576\n",
      "epoch 117 | training loss 28.8316\n",
      "epoch 118 | training loss 28.8027\n",
      "epoch 119 | training loss 28.7808\n",
      "saving weights\n",
      "done saving\n",
      "==================================================\n",
      "epoch 120 | training loss 28.7641\n",
      "epoch 121 | training loss 28.7224\n",
      "epoch 122 | training loss 28.6897\n",
      "epoch 123 | training loss 28.6643\n",
      "epoch 124 | training loss 28.6546\n",
      "saving weights\n",
      "done saving\n",
      "==================================================\n",
      "epoch 125 | training loss 28.5995\n",
      "epoch 126 | training loss 28.5854\n",
      "epoch 127 | training loss 28.5593\n",
      "epoch 128 | training loss 28.5355\n",
      "epoch 129 | training loss 28.5240\n",
      "saving weights\n",
      "done saving\n",
      "==================================================\n",
      "epoch 130 | training loss 28.4845\n",
      "epoch 131 | training loss 28.4604\n",
      "epoch 132 | training loss 28.4336\n",
      "epoch 133 | training loss 28.4176\n",
      "epoch 134 | training loss 28.3899\n",
      "saving weights\n",
      "done saving\n",
      "==================================================\n",
      "epoch 135 | training loss 28.3777\n",
      "epoch 136 | training loss 28.3429\n",
      "epoch 137 | training loss 28.2990\n",
      "epoch 138 | training loss 28.2773\n",
      "epoch 139 | training loss 28.2593\n",
      "saving weights\n",
      "done saving\n",
      "==================================================\n",
      "epoch 140 | training loss 28.2399\n",
      "epoch 141 | training loss 28.2030\n",
      "epoch 142 | training loss 28.1979\n",
      "epoch 143 | training loss 28.1707\n",
      "epoch 144 | training loss 28.1404\n",
      "saving weights\n",
      "done saving\n",
      "==================================================\n",
      "epoch 145 | training loss 28.1318\n",
      "epoch 146 | training loss 28.0981\n",
      "epoch 147 | training loss 28.0788\n",
      "epoch 148 | training loss 28.0560\n",
      "epoch 149 | training loss 28.0193\n",
      "saving weights\n",
      "done saving\n",
      "==================================================\n",
      "epoch 150 | training loss 28.0166\n",
      "epoch 151 | training loss 27.9793\n",
      "epoch 152 | training loss 27.9627\n",
      "epoch 153 | training loss 27.9320\n",
      "epoch 154 | training loss 27.9334\n",
      "saving weights\n",
      "done saving\n",
      "==================================================\n",
      "epoch 155 | training loss 27.8881\n",
      "epoch 156 | training loss 27.8776\n",
      "epoch 157 | training loss 27.8586\n",
      "epoch 158 | training loss 27.8253\n",
      "epoch 159 | training loss 27.8121\n",
      "saving weights\n",
      "done saving\n",
      "==================================================\n",
      "epoch 160 | training loss 27.7862\n",
      "epoch 161 | training loss 27.7563\n",
      "epoch 162 | training loss 27.7346\n",
      "epoch 163 | training loss 27.7061\n",
      "epoch 164 | training loss 27.6780\n",
      "saving weights\n",
      "done saving\n",
      "==================================================\n",
      "epoch 165 | training loss 27.6704\n",
      "epoch 166 | training loss 27.6552\n",
      "epoch 167 | training loss 27.5996\n",
      "epoch 168 | training loss 27.5884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 169 | training loss 27.5838\n",
      "saving weights\n",
      "done saving\n",
      "==================================================\n",
      "epoch 170 | training loss 27.5325\n",
      "epoch 171 | training loss 27.5265\n",
      "epoch 172 | training loss 27.4938\n",
      "epoch 173 | training loss 27.4548\n",
      "epoch 174 | training loss 27.4460\n",
      "saving weights\n",
      "done saving\n",
      "==================================================\n",
      "epoch 175 | training loss 27.4159\n",
      "epoch 176 | training loss 27.4133\n",
      "epoch 177 | training loss 27.3716\n",
      "epoch 178 | training loss 27.3476\n",
      "epoch 179 | training loss 27.3255\n",
      "saving weights\n",
      "done saving\n",
      "==================================================\n",
      "epoch 180 | training loss 27.2941\n",
      "epoch 181 | training loss 27.2744\n",
      "epoch 182 | training loss 27.2775\n",
      "epoch 183 | training loss 27.2534\n",
      "epoch 184 | training loss 27.2152\n",
      "saving weights\n",
      "done saving\n",
      "==================================================\n",
      "epoch 185 | training loss 27.2013\n",
      "epoch 186 | training loss 27.1849\n",
      "epoch 187 | training loss 27.1619\n",
      "epoch 188 | training loss 27.1264\n",
      "epoch 189 | training loss 27.1113\n",
      "saving weights\n",
      "done saving\n",
      "==================================================\n",
      "epoch 190 | training loss 27.0904\n",
      "epoch 191 | training loss 27.0622\n",
      "epoch 192 | training loss 27.0465\n",
      "epoch 193 | training loss 27.0127\n",
      "epoch 194 | training loss 27.0118\n",
      "saving weights\n",
      "done saving\n",
      "==================================================\n",
      "epoch 195 | training loss 26.9868\n",
      "epoch 196 | training loss 26.9717\n",
      "epoch 197 | training loss 26.9394\n",
      "epoch 198 | training loss 26.9140\n",
      "epoch 199 | training loss 26.9088\n",
      "saving weights\n",
      "done saving\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "enc = Encoder(21*12)\n",
    "enc.load_state_dict(torch.load('./checkpoints/enc.pth'))\n",
    "dec1 = Decoder1(64)\n",
    "dec1.apply(init_weights)\n",
    "\n",
    "params = list(dec1.parameters())\n",
    "optim1 = optim.Adam(params, lr=1e-5, weight_decay=0.0)\n",
    "mse_loss = torch.nn.L1Loss(reduction='none')\n",
    "w = torch.tensor([10, 5, 3, 1, 0.5, 0.4, 0.3])\n",
    "w = w/w.sum()\n",
    "\n",
    "for epoch in range(200):\n",
    "    np.random.shuffle(train_dates)\n",
    "    epoch_loss = 0\n",
    "    enc.train()\n",
    "    dec1.train()\n",
    "    loss_t = 0.0\n",
    "    for i in range(len(train_dates)):\n",
    "        x,y = readData(attributes=training_attributes, history=history, date=train_dates[i])\n",
    "        x,y = x.float(), y.float()\n",
    "        embed = enc(x)\n",
    "        y_pred = dec1(embed)\n",
    "        loss_mse = 0.0 \n",
    "        for c in range(len(countries_to_extract)):\n",
    "            loss_mse += (mse_loss(y_pred[c],y[c])*w).mean()\n",
    "        loss = loss_mse\n",
    "        loss /= len(countries_to_extract)\n",
    "        loss.backward()\n",
    "        optim1.step()\n",
    "        optim1.zero_grad()\n",
    "        loss_t += loss.item()\n",
    "    print('epoch %d | training loss %0.4f'%(epoch, loss_t))\n",
    "    \n",
    "    if (epoch+1)%5 == 0:\n",
    "        print('saving weights')\n",
    "        torch.save(dec1.state_dict(), './checkpoints/dec1.pth')\n",
    "        print('done saving')\n",
    "        print('='*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GT\n",
      " tensor([3.3557, 1.7672, 2.3862, 1.0832, 0.2652, 0.1236, 0.1928, 0.1609, 0.1613,\n",
      "        0.2194, 0.1228, 2.3998, 0.5780, 0.7817, 1.5962, 0.9984, 1.1281])\n",
      "using given NPI\n",
      " tensor([1.6954e+00, 1.3272e+00, 2.0652e+00, 1.0936e+00, 5.0876e-01, 1.7097e-01,\n",
      "        3.7676e-02, 1.6899e-02, 1.5145e-01, 1.1265e-03, 4.5441e-01, 1.9157e+00,\n",
      "        7.1283e-01, 5.3871e-01, 1.5428e+00, 1.3036e+00, 1.8252e+00],\n",
      "       grad_fn=<SelectBackward>)\n",
      "using zero NPI\n",
      " tensor([0.0580, 0.0580, 0.0580, 0.0580, 0.0580, 0.0580, 0.0580, 0.0580, 0.0580,\n",
      "        0.0580, 0.0580, 0.0580, 0.0580, 0.0580, 0.0580, 0.0580, 0.0580],\n",
      "       grad_fn=<SelectBackward>)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "enc = Encoder(21*12)\n",
    "enc.load_state_dict(torch.load('./checkpoints/enc.pth'))\n",
    "dec1 = Decoder1(64)\n",
    "dec1.load_state_dict(torch.load('./checkpoints/dec1.pth'))\n",
    "mse_loss = torch.nn.L1Loss(reduction='none')\n",
    "\n",
    "\n",
    "\n",
    "def validation(enc, dec1, dates):\n",
    "    enc.eval()\n",
    "    dec1.eval()\n",
    "    loss_list = [0]*len(countries_to_extract)\n",
    "    for i in range(len(dates)):\n",
    "        x,y = readData(attributes=training_attributes, history=history, date=dates[i])\n",
    "        x,y = x.float(), y.float()\n",
    "        if y.shape[1] != 7:\n",
    "            continue\n",
    "        embed = enc(x)\n",
    "        y_pred = dec1(embed)\n",
    "        print('GT\\n', y[:,0])\n",
    "        print('using given NPI\\n', y_pred[:,0])\n",
    "        embed = enc(x*0)\n",
    "        y_pred = dec1(embed)\n",
    "        print('using zero NPI\\n', y_pred[:,0])\n",
    "        sys.exit()\n",
    "        loss = mse_loss(y_pred, y)[:,0]\n",
    "        loss_list = [loss_list[i]+loss[i].item() for i in range(len(countries_to_extract))]\n",
    "    loss_list = [l/len(dates) for l in loss_list]\n",
    "    return loss_list\n",
    "\n",
    "train_dates = npi_date['Date'][150:300].values\n",
    "loss_list = validation(enc, dec1, train_dates)\n",
    "for i in range(len(countries_to_extract)):\n",
    "    print(i,countries_to_extract[i],loss_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | train_loss 492.5075\n",
      "epoch 1 | train_loss 505.6701\n",
      "epoch 2 | train_loss 520.8059\n",
      "epoch 3 | train_loss 538.1214\n",
      "epoch 4 | train_loss 550.7062\n",
      "saving\n",
      "done saving\n",
      "GT_NPI\n",
      "tensor([3., 2., 2., 3., 1., 2., 2., 4., 2., 2., 2., 4.])\n",
      "prescribed_NPI\n",
      "tensor([2., 1., 1., 2., 1., 1., 1., 3., 1., 2., 1., 3.],\n",
      "       grad_fn=<SliceBackward>)\n",
      "==================================================\n",
      "epoch 5 | train_loss 568.8742\n",
      "epoch 6 | train_loss 588.5715\n",
      "epoch 7 | train_loss 607.8962\n",
      "epoch 8 | train_loss 628.5334\n",
      "epoch 9 | train_loss 645.0157\n",
      "saving\n",
      "done saving\n",
      "GT_NPI\n",
      "tensor([3., 3., 2., 4., 2., 1., 2., 3., 2., 1., 2., 0.])\n",
      "prescribed_NPI\n",
      "tensor([1., 1., 1., 2., 1., 1., 1., 2., 1., 1., 1., 2.],\n",
      "       grad_fn=<SliceBackward>)\n",
      "==================================================\n",
      "epoch 10 | train_loss 664.2870\n",
      "epoch 11 | train_loss 680.8904\n",
      "epoch 12 | train_loss 700.1942\n",
      "epoch 13 | train_loss 716.7489\n",
      "epoch 14 | train_loss 734.9568\n",
      "saving\n",
      "done saving\n",
      "GT_NPI\n",
      "tensor([3., 3., 2., 4., 2., 3., 2., 4., 2., 1., 2., 1.])\n",
      "prescribed_NPI\n",
      "tensor([1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1.],\n",
      "       grad_fn=<SliceBackward>)\n",
      "==================================================\n",
      "epoch 15 | train_loss 752.7133\n",
      "epoch 16 | train_loss 768.5936\n",
      "epoch 17 | train_loss 780.7873\n",
      "epoch 18 | train_loss 796.2878\n"
     ]
    }
   ],
   "source": [
    "\n",
    "enc = Encoder(21*12)\n",
    "enc.load_state_dict(torch.load('./checkpoints/enc.pth'))\n",
    "\n",
    "dec2 = Decoder2(input_dim=64+12,output_dim=21*12)\n",
    "dec2.load_state_dict(torch.load('./checkpoints/dec2_prescriptor5.pth'))\n",
    "\n",
    "optim_3 = optim.Adam(dec2.parameters(), lr=1e-5, weight_decay=1e-5)\n",
    "\n",
    "mse_loss = torch.nn.MSELoss(reduction='none')\n",
    "total_dates = npi_date['Date'][85:300].values\n",
    "\n",
    "# gr_weight = (5+torch.tensor([2,8,1,3,3,4,4,9,6,7,2,13]))/5\n",
    "# gr_weight = gr_weight/gr_weight.max()\n",
    "gr_weight_new = torch.tensor([0.20525048673152924, 0.7103343605995178, 0.5421071648597717, 0.7157260775566101, 0.4592893123626709, 0.8335421681404114, 0.765260636806488, 1.0, 0.33554038405418396, 0.6577407121658325, 0.5881587862968445, 0.866989254951477])\n",
    "gr_weight_hist = gr_weight\n",
    "for i in range(history-1):\n",
    "    gr_weight_hist = torch.cat((gr_weight_hist, gr_weight), dim=0)\n",
    "\n",
    "max_range = torch.tensor([3,3,2,4,2,3,2,4,1,2,1,3])\n",
    "max_range_hist = max_range\n",
    "for i in range(history-1):\n",
    "    max_range_hist = torch.cat((max_range_hist, max_range), dim=0)\n",
    "\n",
    "for epoch in range(300):\n",
    "    dec2.train()\n",
    "    np.random.shuffle(total_dates)\n",
    "    dec2.train()\n",
    "    train_loss = 0.0\n",
    "    for i in range(len(total_dates)-history):\n",
    "        optim_3.zero_grad()\n",
    "        x,y = readData(attributes=training_attributes, history=history, date=total_dates[i])\n",
    "        x_new,_ = readData(attributes=training_attributes, history=history, date=total_dates[i+history])\n",
    "        x,y, x_new = x.float(), y.float(), x_new.float()\n",
    "        embed = enc(x)\n",
    "        attr_weights = torch.rand(embed.shape[0],12)*3\n",
    "        embed_new = torch.cat((embed, attr_weights), dim=1)\n",
    "        x_pred = torch.round(dec2(embed_new))\n",
    "        npi_loss = (x_pred.view(x_pred.shape[0],-1,history).sum(dim=2)*attr_weights).sum()/(len(countries_to_extract)*21)\n",
    "        \n",
    "        loss2 = 0.0\n",
    "        loss3 = 0.0\n",
    "        for c in range(len(countries_to_extract)):\n",
    "            loss2 += mse_loss(x_pred[c],x_new[c]).mean()\n",
    "            loss3 += ((max_range_hist-x_pred[c])*gr_weight_hist).mean()\n",
    "        \n",
    "        loss2 /= len(countries_to_extract) \n",
    "        loss3 /= len(countries_to_extract) \n",
    "        loss = 0.5*loss2 + 0.05*npi_loss + 2.0*loss3 \n",
    "        \n",
    "        loss.backward()\n",
    "        optim_3.step()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    print('epoch %d | train_loss %0.4f'%(epoch, train_loss))\n",
    "    if (epoch+1)%5 == 0:\n",
    "        print('saving')\n",
    "        torch.save(dec2.state_dict(), './checkpoints/dec2_prescriptor6.pth')\n",
    "        print('done saving')\n",
    "        print('GT_NPI')\n",
    "        print(x[0,0:12])\n",
    "        print('prescribed_NPI')\n",
    "        print(x_pred[0,0:12])\n",
    "        print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NPI cost\n",
      " tensor([0.2537, 1.2809, 1.9333, 0.8621, 2.5226, 0.8534, 0.7366, 0.2273, 1.5493,\n",
      "        2.8106, 0.7414, 2.7061])\n",
      "NPI GT\n",
      " tensor([3., 3., 2., 4., 2., 3., 2., 4., 2., 1., 2., 0.], dtype=torch.float64)\n",
      "NPI pred\n",
      " tensor([3., 3., 2., 4., 2., 3., 3., 4., 3., 3., 2., 5.],\n",
      "       grad_fn=<SliceBackward>)\n",
      "==================================================\n",
      "NPI cost\n",
      " tensor([0.6596, 0.7806, 2.4984, 2.7556, 0.1131, 2.3510, 0.8722, 0.6665, 2.0760,\n",
      "        2.1736, 1.1642, 0.8699])\n",
      "NPI GT\n",
      " tensor([3., 3., 2., 4., 1., 2., 2., 3., 2., 3., 1., 1.], dtype=torch.float64)\n",
      "NPI pred\n",
      " tensor([3., 3., 2., 4., 1., 2., 2., 4., 2., 4., 1., 4.],\n",
      "       grad_fn=<SliceBackward>)\n",
      "==================================================\n",
      "NPI cost\n",
      " tensor([0.0294, 2.2114, 2.4352, 2.5469, 1.9960, 2.7121, 1.3715, 2.1045, 2.5396,\n",
      "        1.0447, 0.2097, 2.7707])\n",
      "NPI GT\n",
      " tensor([3., 3., 2., 3., 2., 1., 2., 3., 2., 1., 0., 2.], dtype=torch.float64)\n",
      "NPI pred\n",
      " tensor([3., 4., 2., 4., 3., 3., 3., 4., 3., 3., 2., 5.],\n",
      "       grad_fn=<SliceBackward>)\n",
      "==================================================\n",
      "NPI cost\n",
      " tensor([0.4743, 2.9148, 0.3974, 0.6002, 2.3440, 2.3070, 2.0523, 2.0068, 0.7915,\n",
      "        1.0540, 0.8022, 0.2346])\n",
      "NPI GT\n",
      " tensor([3., 3., 2., 0., 1., 1., 2., 0., 1., 0., 0., 0.], dtype=torch.float64)\n",
      "NPI pred\n",
      " tensor([3., 2., 2., 1., 1., 1., 2., 2., 2., 2., 2., 2.],\n",
      "       grad_fn=<SliceBackward>)\n",
      "==================================================\n",
      "NPI cost\n",
      " tensor([0.5061, 0.1049, 2.5887, 2.0538, 0.8163, 1.6607, 1.5387, 0.8822, 1.3496,\n",
      "        2.9562, 1.2742, 2.9905])\n",
      "NPI GT\n",
      " tensor([3., 3., 2., 4., 0., 1., 2., 4., 2., 3., 1., 0.], dtype=torch.float64)\n",
      "NPI pred\n",
      " tensor([3., 3., 2., 4., 0., 1., 2., 4., 3., 4., 2., 3.],\n",
      "       grad_fn=<SliceBackward>)\n",
      "==================================================\n",
      "NPI cost\n",
      " tensor([2.6762, 2.9004, 1.0845, 2.6962, 2.9628, 0.5703, 1.5494, 2.0430, 2.9379,\n",
      "        1.6955, 0.2885, 1.4253])\n",
      "NPI GT\n",
      " tensor([3., 3., 2., 4., 1., 2., 2., 0., 2., 1., 0., 0.], dtype=torch.float64)\n",
      "NPI pred\n",
      " tensor([2., 2., 1., 3., 0., 1., 1., 0., 1., 1., 0., 1.],\n",
      "       grad_fn=<SliceBackward>)\n",
      "==================================================\n",
      "NPI cost\n",
      " tensor([2.9054, 1.6166, 0.5984, 2.4944, 1.8759, 2.6632, 2.2710, 0.2211, 1.1185,\n",
      "        2.4177, 0.9260, 2.4468])\n",
      "NPI GT\n",
      " tensor([3., 3., 2., 4., 1., 2., 2., 3., 2., 1., 1., 1.], dtype=torch.float64)\n",
      "NPI pred\n",
      " tensor([1., 1., 1., 3., 0., 0., 1., 2., 1., 1., 1., 2.],\n",
      "       grad_fn=<SliceBackward>)\n",
      "==================================================\n",
      "NPI cost\n",
      " tensor([2.5028, 0.8260, 1.3532, 0.1655, 2.8019, 0.9107, 2.8173, 2.2001, 1.0698,\n",
      "        1.4315, 2.1747, 1.9624])\n",
      "NPI GT\n",
      " tensor([3., 2., 2., 1., 1., 2., 1., 4., 2., 1., 1., 0.], dtype=torch.float64)\n",
      "NPI pred\n",
      " tensor([2., 2., 1., 3., 0., 1., 1., 3., 1., 1., 0., 2.],\n",
      "       grad_fn=<SliceBackward>)\n",
      "==================================================\n",
      "NPI cost\n",
      " tensor([1.4360, 0.7394, 1.7779, 2.7548, 2.1579, 2.8364, 2.0318, 2.0634, 1.8584,\n",
      "        1.3485, 1.6674, 1.6500])\n",
      "NPI GT\n",
      " tensor([2., 3., 2., 4., 0., 2., 2., 4., 2., 1., 1., 0.], dtype=torch.float64)\n",
      "NPI pred\n",
      " tensor([1., 2., 1., 3., 0., 1., 1., 3., 2., 2., 2., 2.],\n",
      "       grad_fn=<SliceBackward>)\n",
      "==================================================\n",
      "NPI cost\n",
      " tensor([0.0723, 0.9830, 1.5168, 1.7829, 0.3169, 2.9510, 2.3568, 2.4045, 2.5761,\n",
      "        2.1053, 0.9950, 2.3016])\n",
      "NPI GT\n",
      " tensor([3., 2., 2., 4., 0., 2., 2., 4., 2., 1., 1., 0.], dtype=torch.float64)\n",
      "NPI pred\n",
      " tensor([3., 3., 2., 4., 0., 1., 2., 4., 3., 3., 2., 3.],\n",
      "       grad_fn=<SliceBackward>)\n",
      "==================================================\n",
      "NPI cost\n",
      " tensor([0.1312, 2.6203, 1.2830, 0.7402, 0.7323, 0.9437, 2.3105, 0.8372, 0.9517,\n",
      "        0.7059, 2.5838, 2.4459])\n",
      "NPI GT\n",
      " tensor([3., 3., 2., 3., 0., 2., 1., 3., 2., 1., 1., 0.], dtype=torch.float64)\n",
      "NPI pred\n",
      " tensor([2., 3., 2., 3., 1., 2., 1., 4., 3., 2., 2., 2.],\n",
      "       grad_fn=<SliceBackward>)\n",
      "==================================================\n",
      "NPI cost\n",
      " tensor([1.7919, 1.4869, 1.3017, 0.1867, 0.2195, 0.0483, 2.7635, 2.9352, 1.5970,\n",
      "        1.4635, 2.0845, 0.1908])\n",
      "NPI GT\n",
      " tensor([3., 3., 2., 2., 0., 0., 1., 3., 2., 0., 1., 0.], dtype=torch.float64)\n",
      "NPI pred\n",
      " tensor([2., 2., 1., 2., 0., 1., 1., 3., 2., 1., 0., 3.],\n",
      "       grad_fn=<SliceBackward>)\n",
      "==================================================\n",
      "NPI cost\n",
      " tensor([0.0756, 1.6615, 0.2599, 2.8279, 1.4674, 1.4717, 1.5857, 0.2219, 0.2110,\n",
      "        1.6803, 1.5346, 0.7169])\n",
      "NPI GT\n",
      " tensor([3., 2., 2., 0., 1., 2., 1., 3., 2., 2., 1., 0.], dtype=torch.float64)\n",
      "NPI pred\n",
      " tensor([3., 3., 2., 1., 1., 2., 2., 4., 3., 3., 2., 3.],\n",
      "       grad_fn=<SliceBackward>)\n",
      "==================================================\n",
      "NPI cost\n",
      " tensor([1.1941, 0.7365, 2.5940, 1.0083, 1.4164, 2.6712, 2.2819, 2.7366, 1.4920,\n",
      "        0.2337, 2.8165, 0.7812])\n",
      "NPI GT\n",
      " tensor([2., 1., 2., 2., 0., 1., 0., 3., 2., 1., 1., 0.], dtype=torch.float64)\n",
      "NPI pred\n",
      " tensor([2., 1., 2., 3., 1., 1., 1., 3., 2., 2., 1., 1.],\n",
      "       grad_fn=<SliceBackward>)\n",
      "==================================================\n",
      "NPI cost\n",
      " tensor([1.2563, 2.7896, 1.6742, 1.8932, 0.7347, 0.5394, 0.5975, 2.1847, 1.1871,\n",
      "        1.7656, 2.0453, 1.9317])\n",
      "NPI GT\n",
      " tensor([3., 3., 2., 3., 2., 2., 2., 4., 2., 2., 1., 2.], dtype=torch.float64)\n",
      "NPI pred\n",
      " tensor([3., 3., 2., 3., 1., 2., 2., 3., 2., 2., 1., 3.],\n",
      "       grad_fn=<SliceBackward>)\n",
      "==================================================\n",
      "NPI cost\n",
      " tensor([0.0493, 1.7525, 0.5518, 1.0013, 1.8154, 2.6074, 0.5255, 1.1892, 2.2261,\n",
      "        1.2371, 0.6244, 0.2060])\n",
      "NPI GT\n",
      " tensor([3., 2., 2., 3., 0., 2., 2., 3., 2., 1., 2., 0.], dtype=torch.float64)\n",
      "NPI pred\n",
      " tensor([3., 3., 3., 4., 2., 3., 2., 5., 3., 4., 3., 3.],\n",
      "       grad_fn=<SliceBackward>)\n",
      "==================================================\n",
      "NPI cost\n",
      " tensor([2.5667, 1.3989, 2.7283, 0.3353, 1.1375, 1.5296, 2.2155, 2.1798, 0.7463,\n",
      "        1.8042, 1.5445, 2.6648])\n",
      "NPI GT\n",
      " tensor([3., 3., 2., 4., 1., 3., 2., 4., 2., 1., 1., 0.], dtype=torch.float64)\n",
      "NPI pred\n",
      " tensor([2., 2., 1., 3., 1., 1., 1., 3., 1., 1., 0., 2.],\n",
      "       grad_fn=<SliceBackward>)\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "enc = Encoder(21*12)\n",
    "enc.load_state_dict(torch.load('./checkpoints/enc.pth'))\n",
    "\n",
    "dec2 = Decoder2(input_dim=64+12,output_dim=21*12)\n",
    "dec2.load_state_dict(torch.load('./checkpoints/dec2_prescriptor3.pth'))\n",
    "\n",
    "x,y = readData(attributes=training_attributes, history=history, date=total_dates[50])\n",
    "x_new,y_ = readData(attributes=training_attributes, history=history, date=total_dates[50+21]) # future NPI\n",
    "x,y = x.float(), y.float()\n",
    "attr_weights = torch.rand(embed.shape[0],12)*3\n",
    "x_pred = torch.round(dec2(torch.cat((enc(x), attr_weights), dim=1)))\n",
    "\n",
    "for i in range(17):\n",
    "    print('NPI cost\\n', attr_weights[i])\n",
    "    print('NPI GT\\n', x_new[i,0:12])\n",
    "    print('NPI pred\\n', x_pred[i,0:12])\n",
    "    print(\"=\"*50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
