{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO \n",
    "1. Test for all countries and add country specific attributes. \n",
    "2. Ablation study with **pd.fillna()** and **pd.notna()**. \n",
    "3. Ablation study with and w/o growth_rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch \n",
    "import torch.optim as optim\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "import pandas as pd \n",
    "import os\n",
    "import numpy as np\n",
    "from dateutil.parser import parse \n",
    "\n",
    "def dateConvertor(date):\n",
    "    dt = parse(date)\n",
    "    date = dt.strftime('%Y-%m-%d')\n",
    "    return date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data (for all country)\n",
    "TODO: \n",
    "1. Data cleaning has to be done efficiently. In following code, simply removing Jan, Feb and Dec data and linearly interpolating all NAN. \n",
    "2. If we don't have history, then following program will exit (sys.exit call). Replace this with try and catch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "days | country | attributes =  torch.Size([30, 3, 4]) torch.Size([3])\n",
      "country | days | attributes =  torch.Size([3, 30, 4])\n"
     ]
    }
   ],
   "source": [
    "country_codes = ['ABW','AFG','AGO','ALB','AND','ARE','ARG','AUS','AUT','AZE','BDI','BEL','BEN','BFA','BGD','BGR','BHR','BHS','BIH','BLR','BLZ','BMU','BOL','BRA','BRB','BRN','BTN','BWA','CAF','CAN','CHE','CHL','CHN','CIV','CMR','COD','COG','COL','COM','CPV','CRI','CUB','CYP','CZE','DEU','DJI','DMA','DNK','DOM','DZA','ECU','EGY','ERI','ESP','EST','ETH','FIN','FJI','FRA','FRO','GAB','GBR','GEO','GHA','GIN','GMB','GRC','GRL','GTM','GUM','GUY','HKG','HND','HRV','HTI','HUN','IDN','IND','IRL','IRN','IRQ','ISL','ISR','ITA','JAM','JOR','JPN','KAZ','KEN','KGZ','KHM','KOR','KWT','LAO','LBN','LBR','LBY','LKA','LSO','LTU','LUX','LVA','MAC','MAR','MCO','MDA','MDG','MEX','MLI','MMR','MNG','MOZ','MRT','MUS','MWI','MYS','NAM','NER','NGA','NIC','NLD','NOR','NPL','NZL','OMN','PAK','PAN','PER','PHL','PNG','POL','PRI','PRT','PRY','PSE','QAT','RKS','ROU','RUS','RWA','SAU','SDN','SEN','SGP','SLB','SLE','SLV','SMR','SOM','SRB','SSD','SUR','SVK','SVN','SWE','SWZ','SYC','SYR','TCD','TGO','THA','TJK','TKM','TLS','TTO','TUN','TUR','TWN','TZA','UGA','UKR','URY','USA','UZB','VEN','VIR','VNM','VUT','YEM','ZAF','ZMB','ZWE']\n",
    "filenames = [\"c1_school_closing.csv\", \"c2_workplace_closing.csv\", \"c3_cancel_public_events.csv\", \"c4_restrictions_on_gatherings.csv\", \"c5_close_public_transport.csv\", \"c6_stay_at_home_requirements.csv\", \"c7_movementrestrictions.csv\", \"c8_internationaltravel.csv\", \"confirmed_cases.csv\"]\n",
    "\n",
    "country_code2id = {}\n",
    "for i in range(len(country_codes)):\n",
    "    country_code2id[country_codes[i]] = i \n",
    "\n",
    "# date extraction\n",
    "npi_date = pd.DataFrame({})\n",
    "npi_df = pd.read_csv(os.path.join('timeseries', filenames[0])).T\n",
    "npi_date['Date'] = npi_df.index.values[3:]\n",
    "npi_date['Date'] = npi_date['Date'].apply(dateConvertor)\n",
    "\n",
    "# extract data \n",
    "dataframes = {} \n",
    "countries_to_extract = ['ITA','IND','USA'] # countries code for which you want data. \n",
    "index = [country_code2id[code] for code in countries_to_extract]\n",
    "for file in filenames:\n",
    "    npi_df = pd.read_csv(os.path.join('timeseries', file)).T[3:]\n",
    "    npi_df['Date'] = npi_date['Date'].values\n",
    "    npi_df.set_index('Date', drop=True, inplace=True)\n",
    "    npi_df = npi_df[index] # selecting countries \n",
    "    npi_df = npi_df[64:335] # removing Jan, Feb and Dec data\n",
    "    for col in npi_df:\n",
    "        npi_df[col] = pd.to_numeric(npi_df[col], errors='coerce') # converting object to numeric \n",
    "    npi_df.interpolate(method='linear', inplace=True) # interpolate missing values \n",
    "    dataframes[file[:-4]] = npi_df\n",
    "    \n",
    "    # computing growth rate\n",
    "    if(file[:-4]=='confirmed_cases'):\n",
    "        npi_df = pd.read_csv(os.path.join('timeseries', file)).T[3:]\n",
    "        npi_df['Date'] = npi_date['Date'].values\n",
    "        npi_df.set_index('Date', drop=True, inplace=True)\n",
    "        npi_df = npi_df[index]\n",
    "        npi_df = npi_df[64:335]\n",
    "        for col in npi_df:\n",
    "            npi_df[col] = pd.to_numeric(npi_df[col], errors='coerce')\n",
    "        npi_df = 100*npi_df.diff()/npi_df\n",
    "        npi_df.interpolate(method='linear', inplace=True) # interpolate missing values     \n",
    "        dataframes['growth_rate'] = npi_df.rolling(3).mean()\n",
    "\n",
    "def readData(attributes, history, date):\n",
    "    index = dataframes['c1_school_closing'].index.get_loc(date)\n",
    "    if(history>index):\n",
    "        print('Not sufficient history')\n",
    "        sys.exit()\n",
    "    data = []\n",
    "    for att in attributes:\n",
    "        temp = dataframes[att].iloc[index-history:index].values\n",
    "        if(len(data)==0):\n",
    "            data = np.asarray(temp)\n",
    "        else:\n",
    "            data = np.dstack((data, temp))\n",
    "    x = torch.from_numpy(data).to(dtype=torch.double)\n",
    "    y = torch.from_numpy(dataframes['growth_rate'].iloc[index].values).to(dtype=torch.double)\n",
    "    return x,y\n",
    "\n",
    "training_attributes = ['c1_school_closing', 'c2_workplace_closing', 'c3_cancel_public_events', 'c5_close_public_transport']\n",
    "x,y = readData(attributes=training_attributes, history=30, date='2020-07-15')\n",
    "print('days | country | attributes = ', x.shape, y.shape)\n",
    "print('country | days | attributes = ', x.permute(1,0,2).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data (for single country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\"c1_school_closing.csv\", \"c2_workplace_closing.csv\", \"c3_cancel_public_events.csv\",\n",
    "            \"c4_restrictions_on_gatherings.csv\", \"c5_close_public_transport.csv\", \"c6_stay_at_home_requirements.csv\", \"c7_movementrestrictions.csv\",\n",
    "            \"c8_internationaltravel.csv\", \"confirmed_cases.csv\"]\n",
    "# filenames = np.core.defchararray.add('timeseries/', np.asarray(filenames))\n",
    "npi_data = pd.DataFrame({})\n",
    "\n",
    "# date extraction \n",
    "file = filenames[0]\n",
    "npi_df = pd.read_csv(os.path.join('timeseries', file))\n",
    "npi_df = npi_df[npi_df['country_name']=='India'].iloc[:,3:].T\n",
    "npi_data['Date'] = npi_df[77].index.values\n",
    "npi_data['Index'] = npi_data['Date'].index.values/100.0\n",
    "\n",
    "# other attributes extraction \n",
    "for file in filenames:\n",
    "    npi_df = pd.read_csv(os.path.join('timeseries', file))\n",
    "    npi_df = npi_df[npi_df['country_name']=='India'].iloc[:,3:].T\n",
    "    npi_data[file[:-4]] = npi_df[77].values\n",
    "\n",
    "# compute growth rate \n",
    "npi_data['growth_rate'] = npi_data['confirmed_cases'].diff()\n",
    "npi_data['growth_rate'] = 100*npi_data['growth_rate']/npi_data['confirmed_cases'] # (0,1) or (0,100)\n",
    "\n",
    "# smoothing growth_rate\n",
    "npi_data['growth_rate'] = npi_data['growth_rate'].rolling(3).mean()\n",
    "\n",
    "# cleaning df\n",
    "npi_data = npi_data[64:]\n",
    "# for col in npi_data.columns:\n",
    "#     npi_data= npi_data[npi_data[col].notna()]\n",
    "\n",
    "# interpolating instead of skipping \n",
    "npi_data.interpolate(method='linear', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline linear model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(torch.nn.Module):\n",
    "    def __init__(self, in_=300, out_=1):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(in_, 64)\n",
    "        self.linear2 = torch.nn.Linear(64, 8)\n",
    "        self.linear3 = torch.nn.Linear(8, 1)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1)\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training baseline model without positional info\n",
    "\n",
    "1. Add country specific info here while creating dataset for model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = 30\n",
    "\n",
    "y_total = npi_data['growth_rate'].values\n",
    "x_total = npi_data.drop(columns=['Date', 'confirmed_cases']).to_numpy(dtype=np.float)\n",
    "\n",
    "x_total = torch.from_numpy(x_total).to(dtype=torch.double)\n",
    "y_total = torch.from_numpy(y_total).to(dtype=torch.double)\n",
    "\n",
    "model = LinearModel(x_total.shape[1]*history).to(dtype=torch.double)\n",
    "model.apply(init_weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "split_index = 150\n",
    "\n",
    "x_train = torch.clone(x_total[0:split_index,:])\n",
    "y_train = torch.clone(y_total[0:split_index])\n",
    "x_test = torch.clone(x_total[split_index-history:,:])\n",
    "y_test = torch.clone(y_total[split_index-history:])\n",
    "\n",
    "print('attributes | ', x_train.shape[1])\n",
    "print('training | ', len(x_train))\n",
    "print('test | ', len(x_test)-history)\n",
    "print('-'*20)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "validation_loss = 0.0\n",
    "x_temp = x_test[0:history,:]\n",
    "y_pred = model(x_temp)\n",
    "loss = mse_loss(y_pred, y_test[1])\n",
    "validation_loss += loss.item()\n",
    "for i in range(history+1,len(x_test)-1):\n",
    "    x_temp = x_test[i-history:i,:] # [30,9] --> [1,30*9]\n",
    "    x_temp[-1,-1] = y_pred.item()\n",
    "    y_pred = model(x_temp)\n",
    "    loss = mse_loss(y_pred, y_test[i+1])\n",
    "    validation_loss += loss.item()\n",
    "print('validation loss before training %0.2f' %(validation_loss))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(200):\n",
    "    training_loss = 0.0\n",
    "    x_train = torch.clone(x_total[0:split_index,:])\n",
    "    y_train = torch.clone(y_total[0:split_index])\n",
    "    for i in range(history,len(x_train)-1):\n",
    "        optimizer.zero_grad() # make gradients zero \n",
    "        x_temp = x_train[i-history:i,:]\n",
    "        y_pred = model(x_temp)\n",
    "        loss = mse_loss(y_pred, y_train[i+1])\n",
    "        loss.backward() # computing gradients \n",
    "        optimizer.step() # updating weights \n",
    "        training_loss += loss.item() \n",
    "    if((epoch+1)%5 == 0):\n",
    "        model.eval()\n",
    "        validation_loss = 0.0\n",
    "        x_test = torch.clone(x_total[split_index-history:,:])\n",
    "        y_test = torch.clone(y_total[split_index-history:])\n",
    "        \n",
    "        x_temp = x_test[0:history,:]\n",
    "        y_pred = model(x_temp)\n",
    "        loss = mse_loss(y_pred, y_test[1])\n",
    "        validation_loss += loss.item()\n",
    "        for i in range(history+1,len(x_test)-1):\n",
    "            x_temp = x_test[i-history:i,:]\n",
    "            x_temp[-1,-1] = y_pred.item()\n",
    "            y_pred = model(x_temp)\n",
    "            loss = mse_loss(y_pred, y_test[i+1])\n",
    "            validation_loss += loss.item()\n",
    "        print('epoch %d | training loss %0.2f | validation loss %0.2f'%(epoch, training_loss, validation_loss))\n",
    "        model.train()        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
