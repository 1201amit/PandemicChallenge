{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from dateutil.parser import parse \n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_codes = ['ABW','AFG','AGO','ALB','AND','ARE','ARG','AUS','AUT','AZE','BDI','BEL','BEN','BFA','BGD','BGR','BHR','BHS','BIH','BLR','BLZ','BMU','BOL','BRA','BRB','BRN','BTN','BWA','CAF','CAN','CHE','CHL','CHN','CIV','CMR','COD','COG','COL','COM','CPV','CRI','CUB','CYP','CZE','DEU','DJI','DMA','DNK','DOM','DZA','ECU','EGY','ERI','ESP','EST','ETH','FIN','FJI','FRA','FRO','GAB','GBR','GEO','GHA','GIN','GMB','GRC','GRL','GTM','GUM','GUY','HKG','HND','HRV','HTI','HUN','IDN','IND','IRL','IRN','IRQ','ISL','ISR','ITA','JAM','JOR','JPN','KAZ','KEN','KGZ','KHM','KOR','KWT','LAO','LBN','LBR','LBY','LKA','LSO','LTU','LUX','LVA','MAC','MAR','MCO','MDA','MDG','MEX','MLI','MMR','MNG','MOZ','MRT','MUS','MWI','MYS','NAM','NER','NGA','NIC','NLD','NOR','NPL','NZL','OMN','PAK','PAN','PER','PHL','PNG','POL','PRI','PRT','PRY','PSE','QAT','RKS','ROU','RUS','RWA','SAU','SDN','SEN','SGP','SLB','SLE','SLV','SMR','SOM','SRB','SSD','SUR','SVK','SVN','SWE','SWZ','SYC','SYR','TCD','TGO','THA','TJK','TKM','TLS','TTO','TUN','TUR','TWN','TZA','UGA','UKR','URY','USA','UZB','VEN','VIR','VNM','VUT','YEM','ZAF','ZMB','ZWE']\n",
    "filenames = [\"c1_school_closing.csv\", \"c2_workplace_closing.csv\", \"c3_cancel_public_events.csv\", \"c4_restrictions_on_gatherings.csv\", \"c5_close_public_transport.csv\", \"c6_stay_at_home_requirements.csv\", \"c7_movementrestrictions.csv\", \"c8_internationaltravel.csv\", \"confirmed_cases.csv\"]\n",
    "\n",
    "def dateConvertor(date):\n",
    "    dt = parse(date)\n",
    "    date = dt.strftime('%Y-%m-%d')\n",
    "    return date\n",
    "\n",
    "country_code2id = {}\n",
    "for i in range(len(country_codes)):\n",
    "    country_code2id[country_codes[i]] = i \n",
    "\n",
    "# date extraction\n",
    "npi_date = pd.DataFrame({})\n",
    "npi_date['Date'] = pd.read_csv(os.path.join('timeseries', filenames[0])).keys()[3:]\n",
    "npi_date['Date'] = npi_date['Date'].apply(dateConvertor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ -3.202      -17.8399061    0.939     ]\n [ -2.20910894 -18.06760563   0.64      ]\n [ -2.54750274 -17.72957746   0.894     ]\n [ -3.1925044  -18.3          0.367     ]\n [ -3.19166074 -18.18732394   0.702     ]]\n"
     ]
    }
   ],
   "source": [
    "dataframes = {} \n",
    "\n",
    "countries_to_extract = ['ITA','IND','USA','CHN','BRA','IRN','CAN','GBR',\n",
    "                        'FRA','ESP','BEL','DEU','NLD','MEX','TUR','SWE','ECU','RUS','PER','CHE'] # countries code for which you want data.\n",
    "countries_to_extract = ['AUS','IND','USA','CHN', 'GBR']\n",
    "index = [country_code2id[code] for code in countries_to_extract]\n",
    "static_data = pd.read_csv(os.path.join('timeseries', 'Consolidated.csv')).T[2:][index].T\n",
    "static_data = static_data.drop('Population', axis = 1 )\n",
    "cols_to_norm = ['Density', 'Median Age']\n",
    "static_data[cols_to_norm] = static_data[cols_to_norm].apply(lambda x: (x - x.min()) / x.max()-x.min())\n",
    "static_data = static_data.to_numpy()\n",
    "tmp = static_data[:,3:]\n",
    "final_static_data = static_data[:,0:3].astype(np.float64)\n",
    "#print(final_static_data)\n",
    "for file in filenames:\n",
    "    npi_df = pd.read_csv(os.path.join('timeseries', file)).T[3:]\n",
    "    npi_df['Date'] = npi_date['Date'].values\n",
    "    npi_df.set_index('Date', drop=True, inplace=True)\n",
    "    npi_df = npi_df[index] # selecting countries \n",
    "    npi_df = npi_df[64:335] # removing Jan, Feb and Dec data\n",
    "    for col in npi_df:\n",
    "        npi_df[col] = pd.to_numeric(npi_df[col], errors='coerce') # converting object to numeric \n",
    "    npi_df.interpolate(method='linear', inplace=True) # interpolate missing values \n",
    "    dataframes[file[:-4]] = npi_df\n",
    "   \n",
    "    \n",
    "    \n",
    "    if(file[:-4]=='confirmed_cases'):\n",
    "#         npi_df = pd.read_csv(os.path.join('timeseries', file))\n",
    "#         print(npi_df)\n",
    "        npi_df = pd.read_csv(os.path.join('timeseries', file)).T[3:]\n",
    "        npi_df['Date'] = npi_date['Date'].values\n",
    "        npi_df.set_index('Date', drop=True, inplace=True)\n",
    "        npi_df = npi_df[index] # selecting countries \n",
    "#         npi_df = npi_df[64:335] # removing Jan, Feb and Dec data\n",
    "        for col in npi_df:\n",
    "            npi_df[col] = pd.to_numeric(npi_df[col], errors='coerce')\n",
    "        npi_df = npi_df.interpolate(method='linear') # interpolate missing values     \n",
    "        npi_df = npi_df.rolling(7).mean()\n",
    "        \n",
    "        npi_df = 100*npi_df.diff()/npi_df\n",
    "        npi_df = npi_df[64:335] # removing Jan, Feb and Dec data\n",
    "        \n",
    "        \n",
    "        \n",
    "#         npi_df.interpolate(method='linear', inplace=True) # interpolate missing values     \n",
    "        dataframes['growth_rate'] = npi_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(attributes, history, date):\n",
    "    index = dataframes['c1_school_closing'].index.get_loc(date)\n",
    "    if(history>index):\n",
    "        print('Not sufficient history')\n",
    "        sys.exit()\n",
    "    date = datetime.datetime.strptime(date, \"%Y-%m-%d\")\n",
    "    ref_date = date - datetime.timedelta(21)\n",
    "    if ref_date.month == date.month:\n",
    "        temperature = tmp[:,int(date.month)].reshape((len(countries_to_extract),1)).astype(np.float64)\n",
    "    else:\n",
    "        temperature = ((tmp[:,int(date.month)] + tmp[:,int(ref_date.month)]) / 2).reshape((len(countries_to_extract),1)).astype(np.float64)\n",
    "    data = []\n",
    "    past_growthrates = dataframes['growth_rate'].iloc[index-history:index].values.reshape((len(countries_to_extract),21))\n",
    "    new_final_static_data = np.concatenate((final_static_data,past_growthrates,temperature),axis=1)\n",
    "    for att in attributes:\n",
    "        temp = dataframes[att].iloc[index-history:index].values\n",
    "        if(len(data)==0):\n",
    "            data = np.asarray(temp)\n",
    "        else:\n",
    "            data = np.dstack((data, temp))\n",
    "    x = torch.cat((torch.from_numpy(data).to(dtype=torch.double).permute(1,0,2).view(len(countries_to_extract),-1),\n",
    "                   torch.from_numpy(new_final_static_data).to(dtype=torch.double)),dim = -1)\n",
    "    #x = torch.cat((torch.from_numpy(data).to(dtype=torch.double).permute(1,0,2).view(len(countries_to_extract),-1),\n",
    "                   #torch.from_numpy(past_growthrates).to(dtype=torch.double)),dim = -1)\n",
    "    #x = torch.from_numpy(data).to(dtype=torch.double).permute(1,0,2)\n",
    "    y = torch.from_numpy(dataframes['growth_rate'].iloc[index].values).to(dtype=torch.double)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch \n",
    "import torch.optim as optim\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "import pandas as pd \n",
    "import os\n",
    "import numpy as np\n",
    "from dateutil.parser import parse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim , out_dim = 64):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(input_dim,256)\n",
    "        self.linear2 = torch.nn.Linear(256,512)\n",
    "        self.linear3 = torch.nn.Linear(512,256)\n",
    "        self.linear4 = torch.nn.Linear(256,128)\n",
    "        self.linear5 = torch.nn.Linear(128,out_dim)\n",
    "        self.prelu1   = torch.nn.PReLU()\n",
    "        self.prelu2   = torch.nn.PReLU()\n",
    "        self.prelu3   = torch.nn.PReLU()\n",
    "        self.prelu4   = torch.nn.PReLU()\n",
    "        self.tanh    = torch.nn.Tanh()\n",
    "        self.dropout = torch.nn.Dropout(p=0.0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.prelu1(self.linear1(x))\n",
    "        x = self.dropout(self.prelu2(self.linear2(x)))\n",
    "        x = self.dropout(self.prelu3(self.linear3(x)))\n",
    "        x = self.dropout(self.prelu4(self.linear4(x)))\n",
    "        x = self.tanh(self.linear5(x)/20)*24\n",
    "        return x.squeeze()\n",
    "    \n",
    "class Decoder1(torch.nn.Module):\n",
    "    def __init__(self,input_dim,output_dim=1):\n",
    "        super(Decoder1, self).__init__()\n",
    "        \n",
    "        self.linear1 = torch.nn.Linear(input_dim,32)\n",
    "        self.linear2 = torch.nn.Linear(32,16)\n",
    "        self.linear3 = torch.nn.Linear(16,1)\n",
    "        self.prelu1   = torch.nn.PReLU()\n",
    "        self.prelu2   = torch.nn.PReLU()\n",
    "        self.dropout = torch.nn.Dropout(p=0.2)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.dropout(self.prelu1(self.linear1(x)))\n",
    "        x = self.dropout(self.prelu2(self.linear2(x)))\n",
    "        x = self.linear3(x)\n",
    "        return x.squeeze()\n",
    "\n",
    "class Decoder2(torch.nn.Module):\n",
    "    def __init__(self,input_dim,output_dim=214):\n",
    "        super(Decoder2, self).__init__()\n",
    "        \n",
    "\n",
    "        self.linear1 = torch.nn.Linear(input_dim,64)\n",
    "        self.linear2 = torch.nn.Linear(input_dim,128)\n",
    "        self.linear3 = torch.nn.Linear(128,output_dim)\n",
    "        self.prelu1   = torch.nn.PReLU()\n",
    "        self.prelu2   = torch.nn.PReLU()\n",
    "        self.dropout = torch.nn.Dropout(p=0.2)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.dropout(self.prelu1(self.linear1(x)))\n",
    "        x = self.dropout(self.prelu2(self.linear2(x)))\n",
    "        x = self.relu(self.linear3(x))\n",
    "        return x.squeeze() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([5, 214])\n"
     ]
    }
   ],
   "source": [
    "##training,testing and validation split\n",
    "training_attributes = [\"c1_school_closing\", \"c2_workplace_closing\", \"c3_cancel_public_events\", \"c4_restrictions_on_gatherings\", \"c5_close_public_transport\", \"c6_stay_at_home_requirements\", \"c7_movementrestrictions\", \"c8_internationaltravel\", \"confirmed_cases\"]\n",
    "history = 21\n",
    "train_dates = npi_date['Date'][85:300].values\n",
    "validation_dates = npi_date['Date'][300:335].values\n",
    "x,y = readData(attributes=training_attributes, history=history, date=train_dates[-1])\n",
    "print(x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 training loss = 299.43714535981417 validation loss= 1.8700228463858366\n",
      "1 training loss = 298.2256590947509 validation loss= 0.5964588616043329\n",
      "2 training loss = 245.32903130725026 validation loss= 4.124207653105259\n",
      "3 training loss = 303.7439147271216 validation loss= 8.09232459962368\n",
      "4 training loss = 245.9518735036254 validation loss= 6.466324090957642\n",
      "5 training loss = 209.75338522717357 validation loss= 6.328000590205193\n",
      "6 training loss = 197.28697706758976 validation loss= 2.5759029053151608\n",
      "7 training loss = 252.7118924483657 validation loss= 7.2768916338682175\n",
      "8 training loss = 200.30786969140172 validation loss= 5.540608778595924\n",
      "9 training loss = 198.41396910697222 validation loss= 3.81608247756958\n",
      "['17.29', '16.04', '14.91', '12.38', '11.99', '10.12', '9.13', '7.77', '6.87', '5.66', '4.80', '3.89', '3.49', '2.91', '2.46', '2.14', '1.79', '1.47', '1.28', '1.19', '0.97', '0.80', '0.69', '0.59', '0.65', '0.60', '0.50', '0.46', '0.43', '0.33', '0.27', '0.22', '0.21', '0.21', '0.21', '0.22', '0.21', '0.22', '0.23', '0.27', '0.27', '0.30', '0.31', '0.29', '0.29', '0.26', '0.25', '0.22', '0.20', '0.22', '0.24', '0.21', '0.22', '0.20', '0.19', '0.19', '0.15', '0.13', '0.14', '0.12', '0.12', '0.13', '0.14', '0.14', '0.17', '0.16', '0.18', '0.19', '0.18', '0.18', '0.16', '0.13', '0.13', '0.12', '0.09', '0.09', '0.09', '0.08', '0.08', '0.12', '0.14', '0.16', '0.19', '0.21', '0.23', '0.23', '0.27', '0.27', '0.28', '0.29', '0.32', '0.35', '0.36', '0.43', '0.54', '0.64', '0.74', '0.81', '0.86', '1.19', '1.34', '1.43', '1.59', '1.64', '1.76', '2.13', '2.06', '2.12', '2.14', '2.24', '2.34', '2.50', '2.58', '2.55', '2.64', '2.68', '2.71', '2.91', '2.93', '2.70', '2.79', '2.81', '3.00', '2.92', '2.66', '2.88', '3.08', '3.01', '3.06', '2.88', '2.83', '3.09', '2.80', '2.56', '2.53', '2.30', '2.18', '2.06', '1.82', '1.65', '1.62', '1.51', '1.41', '1.36', '1.28', '1.14', '1.14', '1.00', '0.93', '0.90', '0.79', '0.74', '0.70', '0.62', '0.59', '0.54', '0.48', '0.47', '0.43', '0.40', '0.40', '0.38', '0.36', '0.33', '0.31', '0.30', '0.29', '0.26', '0.23', '0.22', '0.20', '0.20', '0.20', '0.17', '0.15', '0.16', '0.15', '0.13', '0.12', '0.11', '0.10', '0.09', '0.07', '0.07', '0.08', '0.07', '0.06', '0.06', '0.06', '0.06', '0.06', '0.05', '0.05', '0.06', '0.05', '0.06', '0.06', '0.06', '0.07', '0.07', '0.07', '0.08', '0.08', '0.08', '0.07', '0.07', '0.06', '0.05', '0.06', '0.05', '0.05', '0.06', '0.06', '0.07', '0.07']\n",
      "['1.39', '9.37', '7.49', '1.15', '9.91', '7.44', '8.27', '6.99', '10.26', '4.86', '2.29', '4.53', '1.42', '1.74', '2.42', '3.17', '0.43', '0.43', '0.18', '0.32', '0.09', '-0.03', '0.30', '0.58', '0.26', '0.22', '0.17', '0.13', '-0.06', '0.00', '0.47', '0.05', '-0.04', '0.39', '0.92', '0.43', '0.28', '0.63', '0.12', '0.70', '-0.08', '0.52', '0.69', '0.11', '0.22', '0.15', '0.25', '0.46', '0.36', '0.30', '0.78', '0.65', '0.29', '0.23', '0.46', '-0.15', '0.43', '0.00', '-0.09', '0.60', '0.44', '0.15', '0.35', '0.40', '0.41', '0.37', '0.30', '0.34', '0.02', '0.50', '0.53', '0.22', '0.22', '0.24', '-0.09', '0.09', '0.44', '0.09', '0.05', '0.05', '0.34', '0.41', '0.49', '0.42', '0.26', '0.38', '0.13', '0.00', '0.32', '0.25', '0.06', '0.14', '0.33', '0.02', '0.36', '0.08', '0.03', '0.22', '-0.08', '0.13', '-0.04', '-0.05', '-0.03', '0.10', '0.24', '0.45', '0.12', '0.13', '0.29', '0.34', '0.10', '0.50', '0.32', '0.68', '0.56', '0.40', '0.25', '0.22', '0.55', '0.39', '0.51', '0.60', '0.35', '0.61', '0.35', '1.03', '0.81', '0.57', '0.58', '0.71', '0.00', '0.40', '0.36', '0.41', '0.79', '0.23', '0.44', '1.26', '0.36', '1.13', '1.31', '1.08', '1.31', '0.84', '1.06', '1.21', '0.68', '1.24', '1.01', '1.05', '1.89', '1.04', '1.26', '1.47', '1.60', '1.49', '1.87', '1.27', '0.85', '0.36', '1.03', '0.75', '-0.43', '0.54', '0.55', '1.15', '0.42', '1.27', '1.21', '0.97', '1.11', '0.39', '1.01', '0.70', '0.14', '1.36', '0.66', '0.36', '0.79', '0.93', '0.11', '0.79', '0.82', '0.24', '0.75', '0.69', '0.76', '0.24', '0.59', '0.37', '0.17', '0.61', '0.46', '0.47', '0.33', '0.44', '0.60', '0.23', '0.38', '0.25', '0.32', '0.05', '0.08', '0.04', '0.12', '0.15', '0.37', '0.42', '0.14', '0.22', '0.11', '0.55', '0.12', '0.28', '0.32']\n",
      "==================================================\n",
      "10 training loss = 187.2750929556787 validation loss= 0.6909766830503941\n",
      "11 training loss = 180.46531046926975 validation loss= 1.5089191496372223\n",
      "12 training loss = 198.83630841225386 validation loss= 5.883825778961182\n",
      "13 training loss = 211.4377474784851 validation loss= 3.0254268646240234\n",
      "14 training loss = 201.38804997503757 validation loss= 2.087203424423933\n",
      "15 training loss = 200.0904314815998 validation loss= 6.6479088962078094\n",
      "16 training loss = 217.77561677992344 validation loss= 6.816029116511345\n",
      "17 training loss = 200.7987121269107 validation loss= 7.292213723063469\n",
      "18 training loss = 219.81394240260124 validation loss= 3.9150562956929207\n",
      "19 training loss = 257.15188432112336 validation loss= 4.787227675318718\n",
      "['17.29', '16.04', '14.91', '12.38', '11.99', '10.12', '9.13', '7.77', '6.87', '5.66', '4.80', '3.89', '3.49', '2.91', '2.46', '2.14', '1.79', '1.47', '1.28', '1.19', '0.97', '0.80', '0.69', '0.59', '0.65', '0.60', '0.50', '0.46', '0.43', '0.33', '0.27', '0.22', '0.21', '0.21', '0.21', '0.22', '0.21', '0.22', '0.23', '0.27', '0.27', '0.30', '0.31', '0.29', '0.29', '0.26', '0.25', '0.22', '0.20', '0.22', '0.24', '0.21', '0.22', '0.20', '0.19', '0.19', '0.15', '0.13', '0.14', '0.12', '0.12', '0.13', '0.14', '0.14', '0.17', '0.16', '0.18', '0.19', '0.18', '0.18', '0.16', '0.13', '0.13', '0.12', '0.09', '0.09', '0.09', '0.08', '0.08', '0.12', '0.14', '0.16', '0.19', '0.21', '0.23', '0.23', '0.27', '0.27', '0.28', '0.29', '0.32', '0.35', '0.36', '0.43', '0.54', '0.64', '0.74', '0.81', '0.86', '1.19', '1.34', '1.43', '1.59', '1.64', '1.76', '2.13', '2.06', '2.12', '2.14', '2.24', '2.34', '2.50', '2.58', '2.55', '2.64', '2.68', '2.71', '2.91', '2.93', '2.70', '2.79', '2.81', '3.00', '2.92', '2.66', '2.88', '3.08', '3.01', '3.06', '2.88', '2.83', '3.09', '2.80', '2.56', '2.53', '2.30', '2.18', '2.06', '1.82', '1.65', '1.62', '1.51', '1.41', '1.36', '1.28', '1.14', '1.14', '1.00', '0.93', '0.90', '0.79', '0.74', '0.70', '0.62', '0.59', '0.54', '0.48', '0.47', '0.43', '0.40', '0.40', '0.38', '0.36', '0.33', '0.31', '0.30', '0.29', '0.26', '0.23', '0.22', '0.20', '0.20', '0.20', '0.17', '0.15', '0.16', '0.15', '0.13', '0.12', '0.11', '0.10', '0.09', '0.07', '0.07', '0.08', '0.07', '0.06', '0.06', '0.06', '0.06', '0.06', '0.05', '0.05', '0.06', '0.05', '0.06', '0.06', '0.06', '0.07', '0.07', '0.07', '0.08', '0.08', '0.08', '0.07', '0.07', '0.06', '0.05', '0.06', '0.05', '0.05', '0.06', '0.06', '0.07', '0.07']\n",
      "['0.83', '1.20', '2.08', '1.29', '0.51', '1.76', '2.05', '0.35', '0.37', '1.65', '2.13', '2.00', '1.62', '2.85', '2.06', '1.47', '2.86', '2.67', '1.94', '0.47', '0.24', '0.39', '0.16', '0.39', '0.19', '0.35', '-0.00', '0.65', '0.53', '0.30', '0.18', '-0.06', '0.20', '0.69', '0.14', '0.23', '-0.14', '-0.06', '0.07', '0.19', '0.04', '0.15', '-0.04', '0.41', '0.28', '0.33', '0.05', '0.51', '0.70', '0.23', '0.12', '0.37', '0.68', '0.45', '0.53', '0.09', '0.51', '0.18', '1.33', '1.33', '0.32', '0.46', '0.48', '0.52', '0.19', '-0.05', '0.14', '0.26', '0.89', '0.15', '0.34', '0.44', '0.64', '-0.04', '0.79', '0.33', '0.30', '0.11', '0.77', '0.42', '0.35', '0.11', '0.22', '0.18', '-0.16', '0.40', '-0.12', '0.44', '-0.05', '0.26', '-0.09', '0.20', '0.28', '0.41', '0.33', '0.11', '0.47', '0.13', '-0.39', '0.30', '0.39', '0.13', '0.11', '0.12', '0.61', '-0.23', '0.47', '0.37', '0.27', '0.28', '-0.17', '0.50', '-0.05', '0.38', '-0.22', '0.29', '0.66', '0.03', '0.27', '0.42', '-0.07', '0.04', '0.26', '0.48', '0.45', '0.17', '1.13', '0.26', '0.19', '0.13', '0.49', '0.74', '0.67', '0.50', '0.72', '0.45', '0.80', '0.68', '-0.29', '0.11', '0.60', '1.11', '1.30', '0.21', '1.26', '1.11', '0.89', '1.01', '0.83', '0.79', '-0.25', '0.37', '0.69', '0.89', '0.30', '1.03', '1.88', '1.55', '1.75', '0.20', '1.28', '1.45', '1.56', '1.00', '1.71', '1.52', '1.31', '1.14', '0.39', '1.01', '1.20', '0.48', '0.44', '1.33', '0.72', '1.09', '0.34', '0.71', '0.86', '0.72', '0.02', '0.24', '0.68', '0.50', '0.41', '0.39', '0.80', '0.45', '-0.02', '0.60', '0.58', '0.30', '0.32', '0.34', '0.31', '0.21', '0.20', '1.16', '0.26', '0.80', '0.14', '0.60', '0.42', '0.51', '0.79', '-0.45', '0.41', '0.15', '0.12', '0.66', '0.21', '0.04', '0.24', '-0.23', '0.10']\n",
      "==================================================\n",
      "20 training loss = 229.31117034330964 validation loss= 0.5171709917485714\n",
      "21 training loss = 215.0583767220378 validation loss= 5.130525544285774\n",
      "22 training loss = 215.17104840651155 validation loss= 0.6752101071178913\n",
      "23 training loss = 217.94556890428066 validation loss= 2.569236572831869\n",
      "24 training loss = 225.48940950632095 validation loss= 5.9120145589113235\n",
      "25 training loss = 186.8687055259943 validation loss= 1.65543008223176\n",
      "26 training loss = 219.1526051722467 validation loss= 2.4290889464318752\n",
      "27 training loss = 224.7492572069168 validation loss= 2.7714402191340923\n",
      "28 training loss = 199.5092021934688 validation loss= 1.5775888413190842\n",
      "29 training loss = 207.38680117577314 validation loss= 2.0260373689234257\n",
      "['17.29', '16.04', '14.91', '12.38', '11.99', '10.12', '9.13', '7.77', '6.87', '5.66', '4.80', '3.89', '3.49', '2.91', '2.46', '2.14', '1.79', '1.47', '1.28', '1.19', '0.97', '0.80', '0.69', '0.59', '0.65', '0.60', '0.50', '0.46', '0.43', '0.33', '0.27', '0.22', '0.21', '0.21', '0.21', '0.22', '0.21', '0.22', '0.23', '0.27', '0.27', '0.30', '0.31', '0.29', '0.29', '0.26', '0.25', '0.22', '0.20', '0.22', '0.24', '0.21', '0.22', '0.20', '0.19', '0.19', '0.15', '0.13', '0.14', '0.12', '0.12', '0.13', '0.14', '0.14', '0.17', '0.16', '0.18', '0.19', '0.18', '0.18', '0.16', '0.13', '0.13', '0.12', '0.09', '0.09', '0.09', '0.08', '0.08', '0.12', '0.14', '0.16', '0.19', '0.21', '0.23', '0.23', '0.27', '0.27', '0.28', '0.29', '0.32', '0.35', '0.36', '0.43', '0.54', '0.64', '0.74', '0.81', '0.86', '1.19', '1.34', '1.43', '1.59', '1.64', '1.76', '2.13', '2.06', '2.12', '2.14', '2.24', '2.34', '2.50', '2.58', '2.55', '2.64', '2.68', '2.71', '2.91', '2.93', '2.70', '2.79', '2.81', '3.00', '2.92', '2.66', '2.88', '3.08', '3.01', '3.06', '2.88', '2.83', '3.09', '2.80', '2.56', '2.53', '2.30', '2.18', '2.06', '1.82', '1.65', '1.62', '1.51', '1.41', '1.36', '1.28', '1.14', '1.14', '1.00', '0.93', '0.90', '0.79', '0.74', '0.70', '0.62', '0.59', '0.54', '0.48', '0.47', '0.43', '0.40', '0.40', '0.38', '0.36', '0.33', '0.31', '0.30', '0.29', '0.26', '0.23', '0.22', '0.20', '0.20', '0.20', '0.17', '0.15', '0.16', '0.15', '0.13', '0.12', '0.11', '0.10', '0.09', '0.07', '0.07', '0.08', '0.07', '0.06', '0.06', '0.06', '0.06', '0.06', '0.05', '0.05', '0.06', '0.05', '0.06', '0.06', '0.06', '0.07', '0.07', '0.07', '0.08', '0.08', '0.08', '0.07', '0.07', '0.06', '0.05', '0.06', '0.05', '0.05', '0.06', '0.06', '0.07', '0.07']\n",
      "['3.59', '6.50', '3.18', '5.64', '6.27', '0.57', '7.10', '8.17', '2.93', '4.01', '9.16', '3.82', '3.24', '3.52', '0.36', '0.26', '0.13', '0.08', '-0.03', '-0.02', '0.21', '0.41', '0.19', '0.16', '0.39', '0.12', '0.36', '0.22', '0.57', '0.33', '0.19', '-0.01', '0.09', '0.27', '0.06', '0.34', '-0.01', '0.63', '0.23', '0.24', '0.16', '0.45', '0.20', '0.25', '0.26', '0.23', '0.37', '0.19', '0.26', '0.37', '0.41', '0.27', '0.03', '0.46', '0.16', '0.28', '0.12', '0.37', '0.28', '0.26', '0.34', '0.24', '0.68', '0.14', '0.43', '0.04', '0.54', '0.40', '0.12', '0.13', '0.00', '0.18', '0.18', '0.49', '0.64', '0.29', '0.22', '0.08', '0.24', '0.08', '0.25', '0.20', '0.09', '0.35', '-0.06', '0.02', '0.16', '0.14', '0.03', '0.24', '0.10', '0.13', '-0.10', '0.30', '-0.02', '0.05', '0.22', '0.13', '0.27', '-0.01', '0.17', '0.26', '0.17', '0.18', '0.48', '0.55', '0.27', '0.10', '0.02', '0.23', '0.14', '0.36', '0.21', '0.56', '0.41', '0.28', '0.12', '0.43', '0.58', '0.42', '0.15', '0.92', '0.21', '-0.01', '0.17', '0.37', '0.06', '0.59', '0.25', '0.57', '0.45', '0.43', '0.33', '1.55', '0.27', '0.47', '0.12', '1.26', '0.35', '1.26', '0.29', '0.90', '0.18', '0.91', '0.77', '0.60', '1.16', '1.47', '0.80', '1.58', '0.72', '1.90', '0.63', '1.55', '1.30', '1.79', '1.14', '1.44', '0.19', '0.58', '1.71', '1.27', '1.23', '0.62', '0.95', '1.10', '0.75', '0.58', '0.17', '0.97', '0.99', '0.53', '0.70', '0.18', '0.65', '0.52', '0.45', '0.25', '0.37', '0.26', '0.17', '0.47', '0.44', '0.27', '0.12', '0.03', '0.35', '0.36', '0.51', '0.30', '0.16', '0.16', '0.23', '0.52', '0.24', '0.81', '0.06', '0.12', '0.35', '0.08', '0.26', '0.40', '0.17', '0.29', '0.34', '0.04', '-0.01', '0.08', '0.28', '0.15', '0.09', '0.18', '0.14', '0.06', '-0.02']\n",
      "==================================================\n",
      "30 training loss = 200.27604132145643 validation loss= 3.4112683683633804\n",
      "31 training loss = 198.12824350222945 validation loss= 2.0435603857040405\n",
      "32 training loss = 202.13632889837027 validation loss= 2.406708598136902\n",
      "33 training loss = 211.68930008634925 validation loss= 3.0123059451580048\n",
      "34 training loss = 208.74760611727834 validation loss= 4.864217504858971\n",
      "35 training loss = 169.4328600242734 validation loss= 1.915319038555026\n",
      "36 training loss = 188.05544278398156 validation loss= 0.333081079646945\n",
      "37 training loss = 264.53203973174095 validation loss= 0.9540045969188213\n",
      "38 training loss = 273.3697902671993 validation loss= 3.661518692970276\n",
      "39 training loss = 222.99832773953676 validation loss= 2.751996662467718\n",
      "['17.29', '16.04', '14.91', '12.38', '11.99', '10.12', '9.13', '7.77', '6.87', '5.66', '4.80', '3.89', '3.49', '2.91', '2.46', '2.14', '1.79', '1.47', '1.28', '1.19', '0.97', '0.80', '0.69', '0.59', '0.65', '0.60', '0.50', '0.46', '0.43', '0.33', '0.27', '0.22', '0.21', '0.21', '0.21', '0.22', '0.21', '0.22', '0.23', '0.27', '0.27', '0.30', '0.31', '0.29', '0.29', '0.26', '0.25', '0.22', '0.20', '0.22', '0.24', '0.21', '0.22', '0.20', '0.19', '0.19', '0.15', '0.13', '0.14', '0.12', '0.12', '0.13', '0.14', '0.14', '0.17', '0.16', '0.18', '0.19', '0.18', '0.18', '0.16', '0.13', '0.13', '0.12', '0.09', '0.09', '0.09', '0.08', '0.08', '0.12', '0.14', '0.16', '0.19', '0.21', '0.23', '0.23', '0.27', '0.27', '0.28', '0.29', '0.32', '0.35', '0.36', '0.43', '0.54', '0.64', '0.74', '0.81', '0.86', '1.19', '1.34', '1.43', '1.59', '1.64', '1.76', '2.13', '2.06', '2.12', '2.14', '2.24', '2.34', '2.50', '2.58', '2.55', '2.64', '2.68', '2.71', '2.91', '2.93', '2.70', '2.79', '2.81', '3.00', '2.92', '2.66', '2.88', '3.08', '3.01', '3.06', '2.88', '2.83', '3.09', '2.80', '2.56', '2.53', '2.30', '2.18', '2.06', '1.82', '1.65', '1.62', '1.51', '1.41', '1.36', '1.28', '1.14', '1.14', '1.00', '0.93', '0.90', '0.79', '0.74', '0.70', '0.62', '0.59', '0.54', '0.48', '0.47', '0.43', '0.40', '0.40', '0.38', '0.36', '0.33', '0.31', '0.30', '0.29', '0.26', '0.23', '0.22', '0.20', '0.20', '0.20', '0.17', '0.15', '0.16', '0.15', '0.13', '0.12', '0.11', '0.10', '0.09', '0.07', '0.07', '0.08', '0.07', '0.06', '0.06', '0.06', '0.06', '0.06', '0.05', '0.05', '0.06', '0.05', '0.06', '0.06', '0.06', '0.07', '0.07', '0.07', '0.08', '0.08', '0.08', '0.07', '0.07', '0.06', '0.05', '0.06', '0.05', '0.05', '0.06', '0.06', '0.07', '0.07']\n",
      "['0.89', '3.05', '4.25', '1.54', '1.14', '3.05', '1.65', '4.95', '5.13', '3.90', '3.07', '4.99', '2.31', '2.10', '6.02', '3.29', '1.97', '1.78', '0.94', '0.36', '0.91', '0.79', '0.32', '0.22', '0.44', '0.17', '0.09', '0.67', '0.20', '0.30', '-0.07', '0.34', '0.29', '0.20', '0.41', '0.40', '0.36', '0.49', '0.01', '0.37', '0.58', '0.08', '0.23', '0.30', '0.59', '0.38', '0.37', '0.45', '0.34', '-0.12', '0.43', '0.22', '0.45', '0.10', '0.07', '0.21', '0.04', '0.45', '0.25', '0.13', '0.40', '0.85', '0.35', '0.31', '0.24', '0.17', '0.31', '0.14', '0.13', '0.24', '0.60', '0.42', '0.11', '0.31', '0.16', '0.64', '-0.12', '0.39', '-0.14', '0.51', '-0.02', '-0.26', '0.31', '0.27', '0.10', '0.20', '0.10', '0.45', '0.11', '-0.06', '0.34', '0.11', '0.12', '0.14', '0.36', '0.09', '0.10', '-0.05', '-0.03', '0.37', '-0.00', '0.30', '-0.07', '0.30', '0.25', '0.36', '0.07', '-0.09', '0.28', '-0.04', '0.47', '0.90', '0.61', '1.55', '0.20', '1.61', '1.28', '0.74', '1.72', '1.31', '0.71', '0.11', '0.44', '1.87', '0.39', '1.83', '2.17', '2.75', '2.26', '2.14', '1.12', '1.79', '0.63', '3.35', '3.46', '2.95', '3.57', '1.41', '2.97', '0.57', '1.30', '3.24', '2.56', '3.59', '2.37', '3.35', '2.95', '1.81', '2.15', '2.20', '2.34', '0.52', '2.59', '0.72', '1.95', '2.34', '1.12', '3.08', '1.68', '2.52', '2.40', '1.67', '2.17', '1.72', '1.55', '1.36', '0.48', '1.99', '0.13', '1.44', '1.37', '0.60', '0.83', '0.69', '0.81', '1.02', '0.59', '0.56', '0.40', '0.44', '0.68', '0.18', '0.36', '0.29', '0.07', '0.13', '0.04', '0.20', '0.19', '0.41', '0.40', '0.20', '0.23', '0.37', '0.14', '0.20', '0.07', '0.01', '0.32', '-0.18', '0.33', '0.24', '-0.09', '0.15', '0.32', '0.52', '-0.10', '0.34', '0.25', '0.21', '-0.04', '0.29', '0.24', '0.23', '0.02']\n",
      "==================================================\n",
      "40 training loss = 265.9825311899185 validation loss= 2.8031206876039505\n",
      "41 training loss = 264.06680150330067 validation loss= 2.557916786521673\n",
      "42 training loss = 257.91640577837825 validation loss= 1.8467022515833378\n",
      "43 training loss = 257.9322658665478 validation loss= 2.4758549742400646\n",
      "44 training loss = 261.49547175318 validation loss= 3.4200505912303925\n",
      "45 training loss = 262.1677510626614 validation loss= 5.934209242463112\n",
      "46 training loss = 259.89413072913885 validation loss= 6.083488777279854\n",
      "47 training loss = 258.19225676730275 validation loss= 5.577892675995827\n",
      "48 training loss = 260.1394571773708 validation loss= 5.513071432709694\n",
      "49 training loss = 261.6428955793381 validation loss= 5.477981626987457\n",
      "['17.29', '16.04', '14.91', '12.38', '11.99', '10.12', '9.13', '7.77', '6.87', '5.66', '4.80', '3.89', '3.49', '2.91', '2.46', '2.14', '1.79', '1.47', '1.28', '1.19', '0.97', '0.80', '0.69', '0.59', '0.65', '0.60', '0.50', '0.46', '0.43', '0.33', '0.27', '0.22', '0.21', '0.21', '0.21', '0.22', '0.21', '0.22', '0.23', '0.27', '0.27', '0.30', '0.31', '0.29', '0.29', '0.26', '0.25', '0.22', '0.20', '0.22', '0.24', '0.21', '0.22', '0.20', '0.19', '0.19', '0.15', '0.13', '0.14', '0.12', '0.12', '0.13', '0.14', '0.14', '0.17', '0.16', '0.18', '0.19', '0.18', '0.18', '0.16', '0.13', '0.13', '0.12', '0.09', '0.09', '0.09', '0.08', '0.08', '0.12', '0.14', '0.16', '0.19', '0.21', '0.23', '0.23', '0.27', '0.27', '0.28', '0.29', '0.32', '0.35', '0.36', '0.43', '0.54', '0.64', '0.74', '0.81', '0.86', '1.19', '1.34', '1.43', '1.59', '1.64', '1.76', '2.13', '2.06', '2.12', '2.14', '2.24', '2.34', '2.50', '2.58', '2.55', '2.64', '2.68', '2.71', '2.91', '2.93', '2.70', '2.79', '2.81', '3.00', '2.92', '2.66', '2.88', '3.08', '3.01', '3.06', '2.88', '2.83', '3.09', '2.80', '2.56', '2.53', '2.30', '2.18', '2.06', '1.82', '1.65', '1.62', '1.51', '1.41', '1.36', '1.28', '1.14', '1.14', '1.00', '0.93', '0.90', '0.79', '0.74', '0.70', '0.62', '0.59', '0.54', '0.48', '0.47', '0.43', '0.40', '0.40', '0.38', '0.36', '0.33', '0.31', '0.30', '0.29', '0.26', '0.23', '0.22', '0.20', '0.20', '0.20', '0.17', '0.15', '0.16', '0.15', '0.13', '0.12', '0.11', '0.10', '0.09', '0.07', '0.07', '0.08', '0.07', '0.06', '0.06', '0.06', '0.06', '0.06', '0.05', '0.05', '0.06', '0.05', '0.06', '0.06', '0.06', '0.07', '0.07', '0.07', '0.08', '0.08', '0.08', '0.07', '0.07', '0.06', '0.05', '0.06', '0.05', '0.05', '0.06', '0.06', '0.07', '0.07']\n",
      "['0.24', '0.19', '0.34', '0.29', '0.33', '-0.08', '-0.10', '0.30', '0.10', '0.45', '0.20', '0.17', '0.14', '0.20', '0.38', '0.21', '0.06', '0.38', '0.25', '0.21', '0.31', '0.16', '0.16', '0.10', '0.27', '0.48', '0.25', '0.32', '0.28', '0.35', '0.37', '0.26', '0.15', '0.42', '0.28', '0.34', '0.18', '0.24', '0.41', '0.25', '0.24', '0.22', '0.42', '0.30', '0.04', '0.30', '0.32', '0.45', '0.40', '0.37', '0.34', '0.17', '0.31', '0.20', '0.28', '0.27', '0.28', '0.25', '0.09', '0.51', '0.25', '0.20', '0.23', '0.31', '0.17', '0.50', '0.27', '0.32', '0.38', '0.37', '0.20', '0.38', '0.34', '0.39', '0.34', '0.27', '0.25', '0.13', '0.17', '0.25', '0.08', '0.29', '0.25', '0.07', '-0.02', '0.13', '-0.00', '0.08', '0.09', '0.11', '0.12', '0.31', '0.16', '0.15', '0.27', '0.22', '0.01', '0.18', '0.17', '-0.03', '0.09', '0.27', '0.15', '0.11', '0.12', '0.03', '0.26', '0.23', '0.15', '0.40', '0.19', '0.15', '-0.07', '-0.10', '0.34', '0.12', '0.22', '0.39', '0.12', '0.31', '0.21', '0.33', '0.17', '0.38', '0.29', '0.27', '0.35', '0.30', '0.15', '0.16', '0.34', '0.32', '0.06', '0.30', '0.28', '0.34', '0.38', '0.29', '0.35', '0.36', '0.27', '0.64', '0.47', '0.17', '0.51', '0.07', '0.52', '0.36', '0.03', '0.45', '-0.05', '0.23', '0.22', '0.26', '0.48', '0.60', '0.50', '0.53', '0.48', '0.17', '0.60', '0.13', '0.41', '0.21', '0.32', '0.33', '0.20', '0.60', '0.54', '0.56', '0.46', '0.34', '0.20', '0.51', '0.45', '0.34', '0.23', '0.46', '0.29', '0.27', '0.54', '0.40', '0.10', '0.59', '0.16', '0.47', '0.04', '0.24', '0.40', '0.43', '0.38', '0.07', '0.32', '0.17', '0.42', '0.29', '0.21', '0.31', '0.35', '0.36', '0.33', '0.38', '0.34', '0.32', '0.24', '0.40', '0.30', '0.29', '0.29', '0.03', '0.33', '0.33', '0.33', '0.24', '0.29']\n",
      "==================================================\n",
      "50 training loss = 258.0651186928153 validation loss= 6.682661235332489\n",
      "51 training loss = 259.47862481325865 validation loss= 6.453400865197182\n",
      "52 training loss = 258.3905762284994 validation loss= 4.91175489127636\n",
      "53 training loss = 259.0419223047793 validation loss= 5.331261321902275\n",
      "54 training loss = 260.0824849642813 validation loss= 6.628151893615723\n",
      "55 training loss = 261.0932825729251 validation loss= 6.318954437971115\n",
      "56 training loss = 260.1695616990328 validation loss= 6.238062992691994\n"
     ]
    }
   ],
   "source": [
    "## Train the predictor first Encoder+ Decoder 1\n",
    "enc = Encoder(21*9+25)\n",
    "dec1 = Decoder1(64)\n",
    "params = list(enc.parameters())+list(dec1.parameters())\n",
    "optim_1 = optim.Adam(params, lr=1e-4, weight_decay=1e-5)\n",
    "mse_loss = torch.nn.L1Loss()\n",
    "best_valid_loss = 10000000000\n",
    "patience = 0\n",
    "\n",
    "for epoch in range(100):\n",
    "    #np.random.shuffle(train_dates)\n",
    "    epoch_loss = 0\n",
    "    enc.train()\n",
    "    dec1.train()\n",
    "    loss_t = 0.0\n",
    "    y_gt = []\n",
    "    y_pred = []\n",
    "    for i in range(len(train_dates)):\n",
    "        \n",
    "        optim_1.zero_grad()\n",
    "        x,y = readData(attributes=training_attributes, history=history, date=train_dates[i])\n",
    "        x,y = x.float(), y.float()\n",
    "        x_temp = x[0].view(-1)\n",
    "        y_temp = y[0]\n",
    "        gr_pred = dec1(enc(x_temp))\n",
    "        loss = mse_loss(gr_pred,y_temp)\n",
    "        loss.backward()\n",
    "        optim_1.step()\n",
    "        loss_t += loss.item()\n",
    "        y_gt.append('%.2f' % y_temp)\n",
    "        y_pred.append('%.2f' % gr_pred)\n",
    "    \n",
    "    enc.eval()\n",
    "    dec1.eval() \n",
    "    valid_loss = 0\n",
    "    for i in range(len(validation_dates)):\n",
    "        x,y = readData(attributes=training_attributes, history=history, date=validation_dates[i])\n",
    "        x,y = x.float(), y.float()\n",
    "        x_temp = x[0].view(-1)\n",
    "        y_temp = y[0]\n",
    "        features = dec1(enc(x_temp))\n",
    "        loss = mse_loss(features,y_temp)\n",
    "        valid_loss += loss.item()\n",
    "    print(epoch, 'training loss =',loss_t,'validation loss=',valid_loss)\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        patience = 0\n",
    "    else:\n",
    "        patience +=1\n",
    "    if patience >=20:\n",
    "        break\n",
    "\n",
    "    if (epoch+1)%10 == 0:\n",
    "        print(y_gt)\n",
    "        print(y_pred)\n",
    "        print('='*50)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train the prescribtor  Decoder 2,freeze encoder and decoder 1\n",
    "## to be written\n",
    "dec2 = Decoder2(64)\n",
    "optim_2 = optim.Adam(dec2.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "for p in enc.parameters():\n",
    "     p.requires_grad = False\n",
    "for p in dec1.parameters():\n",
    "     p.requires_grad = False\n",
    "mse_loss = torch.nn.MSELoss(reduction='none')\n",
    "\n",
    "for epoch in range(1):\n",
    "    np.random.shuffle(train_dates)\n",
    "    epoch_loss = 0\n",
    "    dec2.train()\n",
    "    for i in range(len(train_dates)):\n",
    "        optim_2.zero_grad()\n",
    "        x,y = readData(attributes=training_attributes, history=history, date=train_dates[0])\n",
    "        x = x.float()\n",
    "        npi = dec2(enc(x))\n",
    "        #print(npi.shape)\n",
    "        features = dec1(enc(npi))\n",
    "        #print(features.shape)\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'train_dates' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ded21a107a09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dates' is not defined"
     ]
    }
   ],
   "source": [
    "print(train_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}